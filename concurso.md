---
permalink: /concurso/
layout: page
title: "Desaf√≠o Ita√∫-Binnario"
subheadline: ""
teaser: "Lecciones aprendidas luego de mi primera Competencia."
---
<!-- ...and learn more at the same time. You can message me at the [contact page]({{ site.baseurl }}/contact/). -->

{% comment %} Including the two pictures is a hack, because there's no way use the Foundation Grid (needs a container with row) {% endcomment %}
![picture of me]({{ site.urlimg }}widget1-trofeo-binnario.png){: .left .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}widget1-trofeo-binnario.png){: .center .hide-for-large-up width="500"}
Para los que no saben, esta fue mi primera competencia de Machine Learning, y probablemente la primera para muchos ac√° en Chile. La verdad siempre quise participar, pero no me atrev√≠a, le ten√≠a susto a no dar el ancho, y a pesar de que soy bastante seguro de lo que s√©, uno nunca piensa que puede ganar cuando est√°n compitiendo los mejores.

En Kaggle existe la *"tradici√≥n"* que al finalizar la competencia se comparte la soluci√≥n ganadora (al menos lo que se puede) por parte de los ganadores. Y dado que uno de mis objetivos es desarrollar contenido en Ciencia de Datos en Espa√±ol y porque tengo un esp√≠ritu docente, pretendo contar c√≥mo fue el ganar y qu√© cosas es necesario poner atenci√≥n para que en la comunidad en Chile **vayamos aprendiendo**. Mi intenci√≥n es explicar c√≥mo fue el proceso de competencia y tratando de dar cr√©dito a todos aquellos que fueron parte de este proceso y que no pude agradecer [ac√°](https://www.linkedin.com/feed/update/urn:li:activity:6754831905763471360/).

{% include alert warning='Perd√≥n si doy detalles muy escuetos del modelo, es que, no puedo contar cu√°l fue la soluci√≥n ganadora. Pero tratar√© de contar qu√© lecciones aprend√≠.'%}


Esto parti√≥ un d√≠a de clases en la Academia Desaf√≠o Latam, y mi ayudante [Tamara Zapata](https://www.linkedin.com/in/tamarazapatag/), me dice <q>¬øviste el concurso, deber√≠ay participar, dem√°s ganay?</q> (Demasiado optimista para mi gusto). Nunca lo dije, pero me baj√≥ el susto, ni loco se puede ganar esto, si hay tanta gente inscrita, creo que al final fueron alrededor de 1000 equipos, y mucha gente que sabe. Aparte el premio, USD$20.000, era muy llamativo por lo que era muy probable que muchas personas quisieran participar. M√°s detalles del concurso [ac√°](https://binnario.ai/challenge/-MMDsMov6MVyOl3gDuOB).

Ese d√≠a despu√©s de la clase nos quedamos revisando la data, y lo primero que recuerdo fue: <q>no tengo idea de la m√©trica</q>. Googleando, v√≠ que era una m√©trica bastante famosa en *object detection* y en modelos de recomendaci√≥n, por lo que primero pens√© fue, quiz√°s podr√≠a probar algunos modelos de recomendaci√≥n en particular <mark>Factorization Machines</mark> que era algo que hab√≠a estado leyendo √∫ltimamente.

{% include alert info='Factorization Machines son un tipo de Modelos no muy conocidos y no tan populares al menos en Chile, que permiten utilizar descomposici√≥n en factores latentes, normalmente factorizando matrices para generar modelos de Filtrado Colaborativo.'%}

Luego notamos que los datos no ten√≠an estructura de modelamiento, eran datos tal cual sal√≠an de una base de datos. Por lo tanto hab√≠a un paso previo. Y eso fue todo lo que hicimos.

Bueno, d√≠as despu√©s me di√≥ por probar. Me puse a hacer **EDA**. Revis√© la cantidad de datos, las distintas tablas, igual era un problema grande, entre train y test eran f√°cil 35 millones de registros contando todas las tablas. Y empec√© a sacar conclusiones de la data que hab√≠a y de lo que no hab√≠a. Entonces dije: <q>voy a hacer un modelo r√°pido!</q> me gusta utilizar Random Forest como modelo Baseline, as√≠ que r√°pidamente entren√© un modelo y qued√© en `3er lugar`. 

> Claramente no me lo esperaba, y ah√≠ dije: *"ya, quiz√°s s√© como resolver este problema"*, y si bien la m√©trica dio bien baja, estaba tercero. Al ocurrir eso, pens√© bueno tengo posibilidades de estar dentro de los 10 primeros lugares y en verdad, con presentar frente a Ita√∫ me conformaba.

Ah√≠ hubo un cambio, porque empec√© a destinar varias horas en la noche para modelar. El tema es que entremedio de este proceso me encontraba terminando de escribir mi Memoria, por lo que me encontraba agotado entre mi trabajo, las clases en la Academia, la Memoria y ahora esto. As√≠ que decid√≠ pedir vacaciones para poder relajarme, pasar tiempo con mi familia, pero tambi√©n dedicarme m√°s de lleno a esto.

Esto me permiti√≥ poner full atenci√≥n al modelo en las noches, que es mi peak de concentraci√≥n. Salieron varias ideas, y al cambiar a modelos m√°s potentes empec√© a mejorar. Ya en mi 3era submisi√≥n qued√© en el 1er lugar, y superando la barrera sicol√≥gica del 50%, que a ese nivel de la competencia estaba muuuuy dura (creo que no m√°s de 3 personas hab√≠an pasado el 50%). Ya ah√≠ me d√≠ cuenta de que hab√≠an reales posibilidades y me lo tom√© en serio. Pens√© en utilizar mi Laptop en su m√°xima capacidad aprovechando que me hab√≠a comprado un Legion 5 nuevito de paquete.


## El primer problema, 1era Lecci√≥n: Prepara tu Compu

Me qued√© corto de RAM, y varios modelos ya no pod√≠an correr, estaba creando muchas variables y mi PC no daba (a ese tiempo ten√≠a 16GB de RAM). As√≠ que tom√© la decisi√≥n de jug√°rmela, dije <q>hay que sacar al menos un podio s√≠ o s√≠ para pagar esta RAM, y sub√≠ a 32GB</q>.

El tema es que al cambiar las RAM nos dimos cuenta que mi compu ten√≠a un slot de RAM quemado y no pod√≠a aumentar. Tuve que viajar a Santiago en `cuarentena`, fue redificil, para cobrar la garant√≠a. Lamentablemente no hab√≠an m√°s laptops de las caracter√≠sticas del m√≠o y no hab√≠a una soluci√≥in r√°pida m√°s que devolverlo. Me ofrecieron la devoluci√≥n del dinero y en modo urgente tuve que ir al *Parque Arauco* a llevarme un computador `ya`. 

No estaba el Legion 5, y un <mark>tremendo vendedor</mark>, (se notaba que sab√≠a) me meti√≥ un Legion 7 y me *"engrupi√≥"* con que tra√≠a una RTX 2070. Y no me pude resistir, obviamente este Laptop sali√≥ harto m√°s caro que el Legi√≥n 5, entonces ahora s√≠ necesitaba sacar un podio para pagarlo. El tema es que el compu no era con retiro inmediato si no que se env√≠aba, y bueno saben la reputaci√≥n de los env√≠os no es la mejor por la pandemia, no siempre llegan cuando dicen, pero este cabro me *"recontrajur√≥"* que en 4 d√≠as llegaba. As√≠ que me la jugu√©, aprovech√© esos d√≠as para terminar mi memoria en un compu rasquita que ten√≠a y afortunadamente el Legion 7 lleg√≥ en 3 d√≠as. Reinstal√© todo, y entre la falla de mi Legion 5 hasta la puesta en marcha del Legion 7 pasaron 10 d√≠as sin poder modelar nada. Mantuve el primer lugar, lo cual me ten√≠a preocupado, pero apenas me puse a probar nuevos modelos aparecieron los pesos pesados, casi todos los finalistas empezaron a superar el 50% y me tiraron muy atr√°s en el leaderboard.

## Otro Problema: 2da Lecci√≥n

![picture of me]({{ site.urlimg }}concurso/codigo.jpeg){: .left .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}concurso/codigo.jpeg){: .center .hide-for-large-up width="500"}
Un segundo problema, ya casi en la recta final del concurso fue, bueno, los modelos que gener√© fueron casi todos a la r√°pida, ¬øc√≥mo me ordeno? Y aqu√≠ viene la otra lecci√≥n: <mark>MODULARIZACI√ìN</mark>.

Comenc√© a Modularizar mi c√≥digo y definir distintas partes lo m√°s automatizadas posibles> Igual ten√≠a una parte bien al lote pero que me dio mucho valor que fue el an√°lisis exploratorio, pero luego todo como relojito:

* **Generaci√≥n de Variables**: Me preocup√© de encapsular todos las variables nuevas que requirieran gran proceso de c√°lculo en funciones sumamente flexibles, si necesitaba generar m√°s, s√≥lo variaba par√°metros y eso me permit√≠a gran poder de experimentaci√≥n, generando muchos cambios con s√≥lo modificar algunos par√°metros.

* **C√≥digo de Entrenamiento**: Tambi√©n me preocup√© de modularizar esto, tratar de encapsular el preprocesamiento fue complejo, porque hab√≠an muchos formatos distintos dependiendo del modelo, pero luego los modelos que fueron entregando los mejores resultados se empezaron a acotar y eso me permiti√≥ simplificar el Script.

* **Inferencia**: Este notebook se encargaba de poder generar las predicciones pero adem√°s generar una estrategia de ordenamiento de las predicciones que era parte del problema. Finalmente se transforma la predicci√≥n final en el formato requerido por la plataforma para la predicci√≥n final.

{% include alert success='Ac√° no descubr√≠ nada nuevo, luego siguiendo a uno de mis Youtubers favoritos hoy, Abishek Thakur, y leyendo su [libro](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf), me d√≠ cuenta que el propone un sistema muy similar, pero much√≠simo m√°s robusto que el m√≠o.'%}

## Otra Lecci√≥n: Aprender del error

Luego de muchos experimentos, not√© que no estaba llevando una manera √≥ptima los experimentos. Lo peor de todo es que tengo un tutorial de MLflow, y <mark>NO LO UTILIC√â</mark>. Obviamente esto me llev√≥ a notar que hay muchas herramientas para trackear experimentos, y hay una particular que me est√° cautivando harto y es [Weigths & Biases](https://wandb.ai/).

Definitivamente not√© un gran valor en llevar los resultados del mejor modelo, y eso lo hice, pero tambi√©n despu√©s not√© que hay un gran valor en llevar el registro de los experimentos no tan exitosos, porque me pas√≥ que como prob√© muchas cosas, se me olvidaba que hab√≠a probado. Eso me gener√≥ mucha inseguridad porque lleg√≥ un momento en la competencia que los puntajes se estancaron, en alrededor de un 55%, yo creo que iba 3ro y no ten√≠a muchas m√°s ideas. Y no sab√≠a cu√°les retomar y cu√°les no.

{% include alert alert='Una dificultad que ten√≠a esta competencia es que s√≥lo indicaba el puntaje si √©ste era mejor que el anterior, en caso de no serlo, uno nunca sab√≠a si hab√≠a errado por poquito o estaba muy perdido.'%}

## B√∫squeda de Hiperpar√°metros

![picture of me](https://optuna.org/assets/img/blog-multivariate-tpe.gif){: .right .show-for-large-up .hide-for-print width="500"}
![picture of me](https://optuna.org/assets/img/blog-multivariate-tpe.gif){: .center .hide-for-large-up width="500"}
Una de las cosas que para m√≠ tiene mucho valor es el tema de entender qu√© hacen los hiperpar√°metros en un modelo. En general leo mucho la documentaci√≥n acerca de qu√© significa, y tiendo a depender mucho de `GridSearch`. Voy mezclando esto con tuneo manual, para sacarle el jugo al modelo.

Esto tiene sus ventajas, tengo control total sobre la b√∫squeda de hiperpar√°metros, pero, depende de que yo est√© atento y adem√°s GridSearchCV en Scikit-Learn es muy ineficiente y tiende a ser lento y explotar mucho debido a su consumo de RAM.

Esto me ayud√≥ mucho a poner en pr√°ctica sobre algunas cosas que aprend√≠ hace tiempo, y que nunca hab√≠a tenido que aplicar: `Downcasting` y el uso de `Matrices Sparse`, que en mi caso aplicaba. Esto me ayud√≥ mucho a reducir el tama√±o del dataset de train y permitir que el GridSearch funcionara, pero... hay manera m√°s eficientes.

Para el futuro, pretendo utilizar un framework de Cross Validation m√°s est√°tico, al final es el KFold lo que aumenta mucho el costo de evaluaci√≥n del modelo, por lo tanto, pretendo aplicar estrategias como las de [Abishek](https://www.youtube.com/watch?v=ArygUBY0QXw).

Adem√°s, nunca me hab√≠a dado el tiempo de aprender librer√≠as espec√≠ficas de optimizaci√≥n de Hiperpar√°metros, especialmente porque intent√© aprender `hyper-opt` una vez y la encontr√© muy engorrosa y nunca encontr√© necesario usarla. Obviamente, modelos grandes requieren una estrategia m√°s Bayesiana para buscar de manera m√°s inteligente y encontr√© que `Optuna` es una tremenda herramienta y la estar√© agregando a mi arsenal.

## Soft Skills

![picture of me]({{ site.urlimg }}concurso/present.png){: .right .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}concurso/present.png){: .center .hide-for-large-up width="500"}
Llegando ya a la √∫ltima semana aparecieron los pesos pesados, y yo estaba sin ideas. [Joaqu√≠n](https://www.linkedin.com/in/joaqunv/) y [Malen](https://www.linkedin.com/in/malen-antillanca/) cada vez que sub√≠an un modelo me pasaban y se instauraron r√°pidamente en el primer lugar, y aparecieron tamb√≠en los equipos de Rodrigo y [Paul](https://www.linkedin.com/in/paul-bertens-ai/) que hicieron pocas submisiones pero sub√≠an muy r√°pido. El tema es que decid√≠ ocupar todo lo que se me ocurr√≠a y el d√≠a Jueves antes de finalizar la competencia qued√© en 2do lugar detr√°s de Joaqu√≠n y Malen env√≠ando mi ultima submisi√≥n, y dije: <q>estoy cansado, esto es todo, con tal de presentar estoy contento/</q>. Al levantarme el viernes ya iba 5to, y al final del d√≠a qued√© 8vo que fue mi puesto final, y estuve a punto de quedar fuera. 

{% include alert tip='Ac√° hay que ser muy estrat√©gico, porque no supe como generar mis √∫ltimas submisiones, de hecho Paul y Rodrigo env√≠aron su submisi√≥n final a las 23:51 pasando en √∫ltimo momento a Joaqu√≠n y Malen. Entonces realmente hay que ser estrat√©gico y pensar bien cuando quemar todos tus cartuchos.' %}

Para rematarme, ese d√≠a sali√≥ un reportaje en [`LUN`](https://www.lun.com/Pages/NewsDetail.aspx?dt=2020-12-11&PaginaId=24&bodyid=0) con los ganadores del a√±o pasado, los fundadores de Acacia, Catalina Espinoza, y Abelino Jim√©nez. Ella, Mag√≠ster de la Universidad de Chile, √©l, Doctor en Carnegie Mellon, yo... un egresado de Ingenier√≠a Civil üòî, hablaron de su soluci√≥n con Redes Neuronales y dije... <q>uuuhhhh, en el Pitch no tengo oportunidad... a menos, que haga mi modelo interesante</q>. Y obvio, no todas las competencias se ganan as√≠, pero esta s√≠. La habilidad de comunicar es tan importante como la habilidad para modelar. Creo que la competencia dej√≥ en evidencia que varias personas saben hacer la pega... para m√°s remate, Catalina y Abelino estaban entre los 3 finalistas y hab√≠an Estudiantes de Doctorado en AI, Profesores de Mag√≠ster, gente de Argentina, Per√∫, etc. ¬øC√≥mo ganarles? Bueno, confiando en tu trabajo, entiendo fortalezas y debilidades de tu modelo y haci√©ndolo relevante. Para ello varios consejos:

* **Entendiendo el problema de Negocio**: Intuyendo para qu√© se usan estos modelos y poniendo atenci√≥n a respuestas de los organizadores. Revisar los comentarios de la competencia me ayud√≥ a entender c√≥mo se va a consumir el modelo, y c√≥mo mi modelo pod√≠a ser de valor para las personas del banco.

* **Poniendo atenci√≥n al Slack**: Mucha de las preguntas que otros hicieron me ayudaron a mejorar mi modelo y a preparar mi Pitch final, le√≠ todo el Slack. Adem√°s, leyendo sutiles pistas de los organizadores ayudaron a confiar en que mi modelo ten√≠a potencial. A pesar de quedar 8vo, pod√≠a enfocarme en c√≥mo mi modelo resolv√≠a problemas reales que ellos ten√≠an.

![picture of me]({{ site.urlimg }}concurso/modelo.png){: .left .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}concurso/modelo.png){: .center .hide-for-large-up width="500"}

* **Preparar el Pitch**: Son 4 minutos en los que te juegas la vida, hicimos 3... y es la peor experiencia por la que ha pasado. El coraz√≥n a mil y la verdad es que da mucho nervio. Si te trabas, no alcanzas en el tiempo y en verdad fueron super rigurosos con eso. Trat√© de incluir comentarios que se hicieron en Slack o durante la conversaci√≥n de finalistas tratando de demostrar siempre c√≥mo mi modelo supl√≠a sus necesidades. 

La preparaci√≥n de esos 4 minutos de Pitch me tomaba f√°cil 6 horas, y mucha pr√°ctica, realmente hab√≠a que escoger muy bien qu√© decir y qu√© no decir. Para el √∫ltimo Pitch estaba muy cansado y la verdad es que el d√≠a anterior me fui a acostar sin tenerlo tan bien preparado. 

Pero bueno el d√≠a de la final despert√© a las 7am, y estuve 3 horas practicando el Pitch final (que era a las 10am), pero no quedaba conforme, me trababa, se me olvidaban partes, no s√©, entr√© a la presentaci√≥n sin estar 100% convencido. 

Afortunadamente me fue bien, y cuando me avisaron que mi modelo era el ganador casi me da un ataque, pens√© que me estaban bromeando. Fue bueno ver que el esfuerzo de a√±os, aprendiendo por mi cuenta, no siendo considerado o sencillamente siendo mirado en menos por la falta de mi t√≠tulo al final daba frutos.

Hay harta gente que agradecer en el camino: Mi esposa `Valentina`, que me dio permiso para acostarme tarde por m√°s de un mes, y ella es muy friolenta. Pero desde la `Tam` que me avis√≥ de la competencia, o amigos como `Lautaro`, yo creo que ni cacha, pero √©l me pas√≥ el primer Libro de Machine Learning que le√≠, el [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) de Max Kuhn, o amigos como Nico, que me hablaron de Deep Learning por primera vez, o Mat√≠as (el Dragon Slayer) que siempre me cuestionaba y que ten√≠a que usar Python y me present√≥ t√©cnolog√≠as como Github o Pytorch. **Gracias!**

![picture of me]({{ site.urlimg }}concurso/LogoBinarioBlanco.png){: .right .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}concurso/LogoBinarioBlanco.png){: .center .hide-for-large-up width="500"}

Algunos comentarios finales:
* Es verdad que competir toma mucho tiempo, pero menos de lo que esperaba. Organiz√°ndose bien uno puede compatibilizar trabajo, familia y hobbies como este. Incluso en Kaggle.
* El computador es importante. Tener una buena maquina ayuda, pero si uno crea un c√≥digo inteligente y flexible puede dejar entrenando en la noche y revisar resultados en el d√≠a. Esto me impact√≥ mucho y estoy trabajando en un paquete en Python para facilitar esta parte. Espero pronto tener noticias, de un BETA.
* Practicar lo que predicas... hay que usar alg√∫n sistema de logging, es la √∫nica manera de tener tus experimentos ordenados. Por otra parte, si bien estrategias como RandomSearch o Gridsearch, es necesario moverse a estrategias de b√∫squeda Bayesiana de modo de incrementar la eficacia de encontrar un buen set de hiperpar√°metros.
* Hay que entender bien qu√© modelos usar, y cu√°ndo desertar de una idea que no da frutos. Esto lo veo porque los 3 finalistas utilizamos modelos de Gradient Boosting. Si bien, es posible utilizar otros, yo intent√© lightFM y una red neuronal en `Keras`, pero si no me hacen entrar en el leaderboard, a pesar de que se ve√≠an prometedores hay que renunciar a ellos y seguir con las ideas que s√≠ dan resultado.
* Nunca menospreciar el poder de un buen Speech. Yo no soy bueno para hablar, ni me considero una persona buena para vender ideas. Pero s√≠ soy seguro de lo que s√© hacer. Y eso hay que trasmitirlo. Creo que la raz√≥n por lo que pude ganar es porque encontr√© la necesidad del banco, lo que ellos realmente esperaban del modelo y me aferr√© a eso y lo defend√≠ las 3 rondas de Pitch y result√≥.

Y bueno, espero que esto pueda ser de utilidad para entender que aparte del modelo hay muchos aspectos a los cuales poner atenci√≥n, y ojal√° cuando haya una nueva competencia de [Binnario](https://binnario.ai/), todos podamos ser mejores y que el ganador siga con esta idea de aconsejar al resto.

Voy a estar constantemente compartiendo material que espero sea √∫til para quienes son entusiastas del Machine/Deep Learning y si en algo les puedo apoyar no duden en contactarme.

[**Alfonso**]({{ site.baseurl }}/contact/)





