---
title: "Bitcoin Price Prediction"
subheadline: "Predecir el comportamiento del BitCoin"
teaser: "Creando LSTM en Pytorch Lightning"
# layout: page-fullwidth
usemathjax: true
category: quick
header: no
image:
    thumb: bitcoin/bitcoin.jpg
tags:
- python
- tutorial
published: true
---


![picture of me]({{ site.urlimg }}bitcoin/bitcoin.jpg){: .left .show-for-large-up .hide-for-print width="300"}
![picture of me]({{ site.urlimg }}bitcoin/bitcoin.jpg){: .center .hide-for-large-up width="250"}

Las criptomonedas son un tema apasionante. En el 칰ltimo tiempo se han convertido en el dolor de cabezas de los gamers (tambi칠n de los modeladores de Deep Learning y de Nvidia que no sabe como producir m치s GPUs), ya que han provocado una escasez en el stock de GPUs, pero tambi칠n han ayudado a ganar (y tambi칠n a perder) dinero a mucha gente. Una de las catacter칤sticas m치s llamativas de este mercado es lo impredecibles que son y c칩mo comentarios de gente importante (<mark>t칤o Elon</mark>) pueden generar variaciones muy importantes de manera s칰bita. Hoy intentaremos predecir el comportamiento del precio del Bitcoin utilizando Pytorch Lightning<!--more-->, aplicando una de las que fueron las redes Neuronales m치s famosas antes de la era de los Transformers, las LSTM (Long Short-Term Memory).

Las LSTM son m치s que un tipo de red, se podr칤a decir que es una configuraci칩n de varias redes neuronales cada una con un prop칩sito en particular. Podr칤amos <q>casi</q> llamarla una arquitectura. La principal caracter칤stica que tienen es que poseen memoria, una caracter칤stica que las hace ideal para trabajar con datos secuenciales, ya que puede utilizar datos del pasado para poder <q>predecir el futuro</q>. Las LSTM Son la evoluci칩n de las RNN convencionales ya que introduciendo un **cell state** son capaces de solucionar el problema del vanishing gradient que 칠stas suelen sufrir.

Entender las redes recurrentes puede ser complicado y no es mi intenci칩n explicar en detalle la matem치tica de fondo de este tipo de redes. Pero para quienes les interese, pueden visitar el [Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) que es el lugar donde mejor se explica esta arquitectura (y de donde rob칠 las siguientes im치genes).

B치sicamente, una LSTM tiene distintas etapas (cada una una red neuronal) que se encargan de aprender o desaprender (espero que esta palabra exista) distintas partes. La primera es la *forget gate*. Esta red neuronal se encarga de olvidar, a medida que se entrena decide qu칠 cosas ya no vale la pena recordar en futuras iteraciones.

![picture of me]({{ site.urlimg }}bitcoin/forget_gate.png){: .center}

La *input gate* es la informaci칩n que llega. El prop칩sito de esta red neuronal es aprender qu칠 aspectos son importantes de los nuevos datos de la secuencia

![picture of me]({{ site.urlimg }}bitcoin/input_gate.png){: .center}

Esta informaci칩n nueva debe combinarse con el output del *forget gate* dando paso al *cell state*. El *cell state* ser치 la memoria y es la informaci칩n nueva menos lo que tiene que olvidarse, es decir, toda la informaci칩n que debe ser pasada a la siguiente iteraci칩n.

![picture of me]({{ site.urlimg }}bitcoin/cell_state.png){: .center}

Finalmente el output ser치 una red que decide la salida de la red considerando tanto la entrada (input gate) como la memoria (cell state). La gracia de una LSTM es que cada capa asociada (neurona LSTM) tendr치 un output, pero adem치s ese output pasar치 a la siguiente iteraci칩n como el *hidden state*.

![picture of me]({{ site.urlimg }}bitcoin/output.png){: .center}

Para poder resolver este problema predictivo se ha utilizado data descargada desde Binance que corresponde a la informaci칩n por minuto del precio del Bitcoin, la cual se puede descargar del siguiente [sitio](https://www.cryptodatadownload.com/data/binance).

Al menos en mi caso descargu칠 datos que van desde el 2019 hasta el 5 de Agosto de 2021 (s칤, me demor칠 en poder generar un modelo decente para mostrar).


```python
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
import matplotlib

import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
from multiprocessing import cpu_count

from sklearn.preprocessing import MinMaxScaler
```
{: title="Librer칤as comunes"}


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import torchmetrics
```
{: title="Ecosistema Pytorch"}

```python
%config InlineBackend.figure_format = 'retina'
sns.set(style = 'whitegrid', palette = 'muted', font_scale = 1.2)
HAPPY_COLORS_PALETTE = ['#01BEFE', '#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 6,4
tqdm.pandas()
```
{: title="Configuraci칩n Visual"}


```python
RANDOM_SEED = 42
pl.seed_everything(RANDOM_SEED)
```
{: title="Asegurar reproducibilidad"}

    Global seed set to 42

Lo primero que haremos ser치 entonces importar la data de `Binance`. S칩lo destacar que se hace un parseo de la fecha y se ordena la data por fecha dado su caracter secuencial. Luego se reinicia el 칤ndice para evitar cualquier inconveniente posterior.

```python
df = pd.read_csv('Binance_BTCUSDT_minute.csv', parse_dates = ['date']).sort_values(by = 'date').reset_index(drop = True)
df.shape
```

    (997650, 10)

La cantidad de data extra칤da es bastante. Las redes recurrentes en general son redes lentas de entrenar debido precisamente a su caracter presencial. Para ayudar en el proceso de entrenamiento, evitar ruidos del pasado (aunque podr칤a impactar en la estacionalidad) y permitir que otras personas sin GPU puedan reproducirlo, decid칤 entrenar s칩lo con la informaci칩n de este a침o.

Debido a que la informaci칩n est치 detallada al minuto, la estacionalidad debiera tambi칠n revisarse a ese nivel. La LSTM debiera fijarse en minutos anteriores para predecir el minuto siguiente.

```python
df.query('date > "2021-01-01"', inplace = True)
print(df.shape)
df
```
{: title="Filtro considerando s칩lo el 2021"}
    (306437, 10)


<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>unix</th>
      <th>date</th>
      <th>symbol</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>Volume BTC</th>
      <th>Volume USDT</th>
      <th>tradecount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>691213</th>
      <td>1609459260000</td>
      <td>2021-01-01 00:01:00</td>
      <td>BTC/USDT</td>
      <td>28961.67</td>
      <td>29017.50</td>
      <td>28961.01</td>
      <td>29009.91</td>
      <td>58.477501</td>
      <td>1.695803e+06</td>
      <td>1651</td>
    </tr>
    <tr>
      <th>691214</th>
      <td>1609459320000</td>
      <td>2021-01-01 00:02:00</td>
      <td>BTC/USDT</td>
      <td>29009.54</td>
      <td>29016.71</td>
      <td>28973.58</td>
      <td>28989.30</td>
      <td>42.470329</td>
      <td>1.231359e+06</td>
      <td>986</td>
    </tr>
    <tr>
      <th>691215</th>
      <td>1609459380000</td>
      <td>2021-01-01 00:03:00</td>
      <td>BTC/USDT</td>
      <td>28989.68</td>
      <td>28999.85</td>
      <td>28972.33</td>
      <td>28982.69</td>
      <td>30.360677</td>
      <td>8.800168e+05</td>
      <td>959</td>
    </tr>
    <tr>
      <th>691216</th>
      <td>1609459440000</td>
      <td>2021-01-01 00:04:00</td>
      <td>BTC/USDT</td>
      <td>28982.67</td>
      <td>28995.93</td>
      <td>28971.80</td>
      <td>28975.65</td>
      <td>24.124339</td>
      <td>6.992262e+05</td>
      <td>726</td>
    </tr>
    <tr>
      <th>691217</th>
      <td>1609459500000</td>
      <td>2021-01-01 00:05:00</td>
      <td>BTC/USDT</td>
      <td>28975.65</td>
      <td>28979.53</td>
      <td>28933.16</td>
      <td>28937.11</td>
      <td>22.396014</td>
      <td>6.483227e+05</td>
      <td>952</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>997645</th>
      <td>1628125260000</td>
      <td>2021-08-05 01:01:00</td>
      <td>BTC/USDT</td>
      <td>39592.99</td>
      <td>39600.00</td>
      <td>39559.77</td>
      <td>39570.02</td>
      <td>33.148678</td>
      <td>1.312017e+06</td>
      <td>1462</td>
    </tr>
    <tr>
      <th>997646</th>
      <td>1628125320000</td>
      <td>2021-08-05 01:02:00</td>
      <td>BTC/USDT</td>
      <td>39570.01</td>
      <td>39570.01</td>
      <td>39388.00</td>
      <td>39396.36</td>
      <td>207.697394</td>
      <td>8.198434e+06</td>
      <td>4960</td>
    </tr>
    <tr>
      <th>997647</th>
      <td>1628125380000</td>
      <td>2021-08-05 01:03:00</td>
      <td>BTC/USDT</td>
      <td>39396.36</td>
      <td>39440.69</td>
      <td>39363.55</td>
      <td>39434.95</td>
      <td>121.355510</td>
      <td>4.780575e+06</td>
      <td>3329</td>
    </tr>
    <tr>
      <th>997648</th>
      <td>1628125440000</td>
      <td>2021-08-05 01:04:00</td>
      <td>BTC/USDT</td>
      <td>39434.95</td>
      <td>39436.77</td>
      <td>39364.06</td>
      <td>39369.67</td>
      <td>46.405522</td>
      <td>1.827913e+06</td>
      <td>1604</td>
    </tr>
    <tr>
      <th>997649</th>
      <td>1628125500000</td>
      <td>2021-08-05 01:05:00</td>
      <td>BTC/USDT</td>
      <td>39369.67</td>
      <td>39394.00</td>
      <td>39338.53</td>
      <td>39361.02</td>
      <td>38.950155</td>
      <td>1.533056e+06</td>
      <td>2341</td>
    </tr>
  </tbody>
</table>
<p>306437 rows 칑 10 columns</p>
</div>

Como se puede apreciar hay 16 variables. Se escogeran las que que ami parecer son m치s relevantes y crearemos algunas que nos permitan captar informaci칩n temporal:

{% include alert warning='No soy para nada experto en temas de Trading. Mi 칰nica intenci칩n con este post es mostrar las capacidades que puede tener una red Neuronal para predecir el comportamiento del Bitcoin. Si este tema le interesa desde el punto de vista financiero o de trading le recomiendo seguir a [Lautaro Parada](https://www.linkedin.com/in/lautaro-parada-opazo-a85615b2/?originalSubdomain=cl), este cabro sabe y ha desarrollado algunas herramientas en Python para facilitar el tema financiero (ver m치s detalles [ac치](https://github.com/LautaroParada/eod-data))'%}

## Creaci칩n del Close Change

Esta es una variable que creo que puede ser significativa. Corresponde a cu치nto cambi칩 el precio del Bitcoin respecto al periodo anterior (en este caso al minuto anterior).

```python
%%time
df['prev_close'] = df.shift(1)['close']
df['close_change'] = (df.close-df.prev_close).fillna(0)
df.head()
```


<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>unix</th>
      <th>date</th>
      <th>symbol</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>Volume BTC</th>
      <th>Volume USDT</th>
      <th>tradecount</th>
      <th>prev_close</th>
      <th>close_change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>691213</th>
      <td>1609459260000</td>
      <td>2021-01-01 00:01:00</td>
      <td>BTC/USDT</td>
      <td>28961.67</td>
      <td>29017.50</td>
      <td>28961.01</td>
      <td>29009.91</td>
      <td>58.477501</td>
      <td>1.695803e+06</td>
      <td>1651</td>
      <td>NaN</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>691214</th>
      <td>1609459320000</td>
      <td>2021-01-01 00:02:00</td>
      <td>BTC/USDT</td>
      <td>29009.54</td>
      <td>29016.71</td>
      <td>28973.58</td>
      <td>28989.30</td>
      <td>42.470329</td>
      <td>1.231359e+06</td>
      <td>986</td>
      <td>29009.91</td>
      <td>-20.61</td>
    </tr>
    <tr>
      <th>691215</th>
      <td>1609459380000</td>
      <td>2021-01-01 00:03:00</td>
      <td>BTC/USDT</td>
      <td>28989.68</td>
      <td>28999.85</td>
      <td>28972.33</td>
      <td>28982.69</td>
      <td>30.360677</td>
      <td>8.800168e+05</td>
      <td>959</td>
      <td>28989.30</td>
      <td>-6.61</td>
    </tr>
    <tr>
      <th>691216</th>
      <td>1609459440000</td>
      <td>2021-01-01 00:04:00</td>
      <td>BTC/USDT</td>
      <td>28982.67</td>
      <td>28995.93</td>
      <td>28971.80</td>
      <td>28975.65</td>
      <td>24.124339</td>
      <td>6.992262e+05</td>
      <td>726</td>
      <td>28982.69</td>
      <td>-7.04</td>
    </tr>
    <tr>
      <th>691217</th>
      <td>1609459500000</td>
      <td>2021-01-01 00:05:00</td>
      <td>BTC/USDT</td>
      <td>28975.65</td>
      <td>28979.53</td>
      <td>28933.16</td>
      <td>28937.11</td>
      <td>22.396014</td>
      <td>6.483227e+05</td>
      <td>952</td>
      <td>28975.65</td>
      <td>-38.54</td>
    </tr>
  </tbody>
</table>
</div>

# Creaci칩n de Variables temporales.

Estas variables son las que nos permitir치n poder determinar posibles estacionalidades. Si bien es posible determinar much칤simas subdivisiones de tiempo, decid칤 que el d칤a de la semana, el d칤a del mes, la semana del a침o y el mes ser칤an suficiente.

```python
df = df.assign(
    day_of_week = lambda x: x.date.dt.dayofweek,
    day_of_month = lambda x: x.date.dt.day,
    week_of_year = lambda x: x.date.dt.isocalendar().week,
    month = lambda x: x.date.dt.month
        )
df.head()
```

{% include alert tip='Es muy probable que aqu칤 est칠 cometiendo un error, ya que estoy entrenando con menos de un a침o y puede que est칠 desaprovechando algunas de estas variables temporales. Inicialmente entren칠 con toda la data pero se demoraba demasiado (aunque s칩lo prob칠 en el notebook). Es importante destacar que este proceso de entrenamiento es largo a pesar de que la data es muy poquita.'%}


<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>unix</th>
      <th>date</th>
      <th>symbol</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>Volume BTC</th>
      <th>Volume USDT</th>
      <th>tradecount</th>
      <th>prev_close</th>
      <th>close_change</th>
      <th>day_of_week</th>
      <th>day_of_month</th>
      <th>week_of_year</th>
      <th>month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>691213</th>
      <td>1609459260000</td>
      <td>2021-01-01 00:01:00</td>
      <td>BTC/USDT</td>
      <td>28961.67</td>
      <td>29017.50</td>
      <td>28961.01</td>
      <td>29009.91</td>
      <td>58.477501</td>
      <td>1.695803e+06</td>
      <td>1651</td>
      <td>NaN</td>
      <td>0.00</td>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>691214</th>
      <td>1609459320000</td>
      <td>2021-01-01 00:02:00</td>
      <td>BTC/USDT</td>
      <td>29009.54</td>
      <td>29016.71</td>
      <td>28973.58</td>
      <td>28989.30</td>
      <td>42.470329</td>
      <td>1.231359e+06</td>
      <td>986</td>
      <td>29009.91</td>
      <td>-20.61</td>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>691215</th>
      <td>1609459380000</td>
      <td>2021-01-01 00:03:00</td>
      <td>BTC/USDT</td>
      <td>28989.68</td>
      <td>28999.85</td>
      <td>28972.33</td>
      <td>28982.69</td>
      <td>30.360677</td>
      <td>8.800168e+05</td>
      <td>959</td>
      <td>28989.30</td>
      <td>-6.61</td>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>691216</th>
      <td>1609459440000</td>
      <td>2021-01-01 00:04:00</td>
      <td>BTC/USDT</td>
      <td>28982.67</td>
      <td>28995.93</td>
      <td>28971.80</td>
      <td>28975.65</td>
      <td>24.124339</td>
      <td>6.992262e+05</td>
      <td>726</td>
      <td>28982.69</td>
      <td>-7.04</td>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>691217</th>
      <td>1609459500000</td>
      <td>2021-01-01 00:05:00</td>
      <td>BTC/USDT</td>
      <td>28975.65</td>
      <td>28979.53</td>
      <td>28933.16</td>
      <td>28937.11</td>
      <td>22.396014</td>
      <td>6.483227e+05</td>
      <td>952</td>
      <td>28975.65</td>
      <td>-38.54</td>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

Finalmente me qued칠 con las siguientes variables. La 칰nica l칩gica que utilic칠 para escoger estas variables es mi sentido com칰n y mi vago conocimiento de finanzas. Si alguien considera que algo m치s debi칩 ser considerado (o que de frent칩n me qequivoqu칠) por favor no dude en notificarlo en mi [github](https://github.com/datacubeR/datacubeR.github.io/issues).

```python
features_df = df[['day_of_week','day_of_month','week_of_year','month', 'open','high','low','close_change','close']].copy()
features_df
```

<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>day_of_week</th>
      <th>day_of_month</th>
      <th>week_of_year</th>
      <th>month</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close_change</th>
      <th>close</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>691213</th>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
      <td>28961.67</td>
      <td>29017.50</td>
      <td>28961.01</td>
      <td>0.00</td>
      <td>29009.91</td>
    </tr>
    <tr>
      <th>691214</th>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
      <td>29009.54</td>
      <td>29016.71</td>
      <td>28973.58</td>
      <td>-20.61</td>
      <td>28989.30</td>
    </tr>
    <tr>
      <th>691215</th>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
      <td>28989.68</td>
      <td>28999.85</td>
      <td>28972.33</td>
      <td>-6.61</td>
      <td>28982.69</td>
    </tr>
    <tr>
      <th>691216</th>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
      <td>28982.67</td>
      <td>28995.93</td>
      <td>28971.80</td>
      <td>-7.04</td>
      <td>28975.65</td>
    </tr>
    <tr>
      <th>691217</th>
      <td>4</td>
      <td>1</td>
      <td>53</td>
      <td>1</td>
      <td>28975.65</td>
      <td>28979.53</td>
      <td>28933.16</td>
      <td>-38.54</td>
      <td>28937.11</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>997645</th>
      <td>3</td>
      <td>5</td>
      <td>31</td>
      <td>8</td>
      <td>39592.99</td>
      <td>39600.00</td>
      <td>39559.77</td>
      <td>-22.97</td>
      <td>39570.02</td>
    </tr>
    <tr>
      <th>997646</th>
      <td>3</td>
      <td>5</td>
      <td>31</td>
      <td>8</td>
      <td>39570.01</td>
      <td>39570.01</td>
      <td>39388.00</td>
      <td>-173.66</td>
      <td>39396.36</td>
    </tr>
    <tr>
      <th>997647</th>
      <td>3</td>
      <td>5</td>
      <td>31</td>
      <td>8</td>
      <td>39396.36</td>
      <td>39440.69</td>
      <td>39363.55</td>
      <td>38.59</td>
      <td>39434.95</td>
    </tr>
    <tr>
      <th>997648</th>
      <td>3</td>
      <td>5</td>
      <td>31</td>
      <td>8</td>
      <td>39434.95</td>
      <td>39436.77</td>
      <td>39364.06</td>
      <td>-65.28</td>
      <td>39369.67</td>
    </tr>
    <tr>
      <th>997649</th>
      <td>3</td>
      <td>5</td>
      <td>31</td>
      <td>8</td>
      <td>39369.67</td>
      <td>39394.00</td>
      <td>39338.53</td>
      <td>-8.65</td>
      <td>39361.02</td>
    </tr>
  </tbody>
</table>
<p>306437 rows 칑 9 columns</p>
</div>

# Data Split

Como se trata de data secuencial el split de datos de validaci칩n no se puede (ni se debe) realizar de manera aleatoria. Si no m치s bien, toda la data, que llamaremos test, tiene que ser data posterior al proceso de entrenamiento siguiendo la secuencia l칩gica temporal.

Es por eso que decid칤 entrenar con el 90% de la data disponible y validar con el 10% restante:


```python
train_size = int(len(features_df)* 0.9)
print('Number of Train Sequences: ', train_size)
train_df, test_df = features_df[:train_size], features_df[train_size + 1:] # + 1 is not necessary
train_df.shape, test_df.shape
```
{: title="Split de la Data"}

    Number of Train Sequences:  275793

    ((275793, 9), (30643, 9))


# Estandarizaci칩n 
Finalmente para lograr una mejor convergencia de los datos generaremos una estadarizaci칩n. Llevando todos los datos al rango -1, 1. 


```python
scaler = MinMaxScaler(feature_range=(-1,1))
train_df = pd.DataFrame(
    scaler.fit_transform(train_df),
    index = train_df.index,
    columns = train_df.columns
    )

test_df = pd.DataFrame(
    scaler.transform(test_df),
    index = test_df.index,
    columns = test_df.columns
    )
```

## Creaci칩n de Secuencias

El formato inicial de la data no viene listo para ser usado en redes recurrentes. Como dije anteriormente, la aplicaci칩n de este tipo de Redes Neuronales se justifica siempre y cuando tengamos data secuencial. Esta data se construir치 de la siguiente manera:

```python
def create_sequences(input_data: pd.DataFrame, target_column, sequence_length):
    sequences = []
    data_size = len(input_data)
    
    for i in tqdm(range(data_size - sequence_length)):
        sequence = input_data[i: i+sequence_length] # No considero el 칰ltimo valor
        label_position = i + sequence_length # Este es el 칰ltimo valor, usado de Label
        label = input_data.iloc[label_position][target_column]
        
        sequences.append((sequence, label))
    
    return sequences
```
{: title="Funci칩n para crear secuencias"}

Esta funci칩n puede ser un poco dificil de explicar por lo que lo har칠 mediante un ejemplo. Supongamos el siguiente DataFrame:

```python
sample_data = pd.DataFrame(dict(
    feature_1 = [1,2,3,4,5],
    label = [6,7,8,9,10]
))
sample_data
```

<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_1</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>9</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>


Ejecutaremos la funci칩n de manera de crear secuencias de largo 3:
```python
sample_sequences = create_sequences(sample_data, 'label', sequence_length=3)
```

Como se puede ver, esta funci칩n dejar치 3 filas (largo de secuencia 3) para cada una de las variables contenidas en el DataFrame. Esos 3 registros tendr치n como objetivo predecir el valor siguiente como se muestra en los siguientes ejemplos:

```python
print()
print(f'label: {sample_sequences[0][1]}')
sample_sequences[0][0]
```
{: title="Primera secuencia"}
    
    label: 9


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_1</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>

Debido a la definici칩n de nuestra funci칩n s칩lo se pueden crear secuencias hasta que el 칰ltimo registro del dataframe exista. Por ejemplo la secuencia para los registros 2, 3, 4 no existe porque no es posible asociarle un label.


```python
print()
print(f'label: {sample_sequences[1][1]}')
sample_sequences[1][0]
```
{: title="Segunda Secuencia"}

    
    label: 10


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_1</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>

{% include alert info='El modelo que queremos plantear ac치 es un modelo de Serie de Tiempo Multivariada. Una de las ventajas que poseen las redes neuronales es que es posible utilizar m치s de una variable (o lo que se llama tambi칠n usar variables ex칩genas) para predecir el comportamiento de mi objetivo en el tiempo.'%}

Por lo tanto para poder crear este modelo predictivo usaremos secuencias de largo 120, es decir, dos horas. 

```python
SEQUENCE_LENGTH = 120
train_sequences = create_sequences(train_df, 'close', SEQUENCE_LENGTH)
test_sequences = create_sequences(test_df, 'close', SEQUENCE_LENGTH)
```

```python
print('Secuencias de Entrenamiento', len(train_sequences))
print('Diferencia entre Registros y Secuencias creadas: ', len(train_df) - len(train_sequences))

```

    Secuencias de Entrenamiento 275673
    Diferencia entre Registros y Secuencias creadas:  120

{% include alert tip='La diferencia entre el n칰mero de registros de un dataset y el n칰mero de secuencias creadas debe ser igual al largo de la secuencia. Estos valores corresponder치n a la secuencia que no se puede crear debido a no poder crear el target.'%}


# Modelo 

El modelo ser치 construido en Pytorch Lightning debido a la facilidad de uso. Ya hemos vistos en post anteriores que cualquier modelo puede ser transformado en Lightning. La ventaja de Lightning es que el c칩digo quedar치 mucho m치s organizado y es posible configurar opciones avanzadas de manera mucho m치s sencilla. 

Si quieres aprender la equivalencia entre un modelo en Pytorch Nativo y uno en Pytorch Lightning puedes ver este [post]({{ site.baseurl }}/lightning).

## Pytorch Dataset y Pytorch Data Module

A continuaci칩n crearemos el Pytorch Dataset, que se encargar치 de generar las secuencias, las cuales ser치n el DataFrame como se explic칩 anteriormente y el label que ser치 el valor a predecir.
Adicionalmente se crear치n los Dataloaders en el Lightning Data Module. Los 3 dataloaders creados ser치n el de entrenamiento, el de validaci칩n y el de predicci칩n. El dataloader de predicci칩n se utilizar치 con la data de testeo que es con la que verificaremos si nuestro modelo responde como esperamos.


```python
class BTCDataset(Dataset):
    def __init__(self, sequences):
        self.sequences = sequences
        
    def __len__(self):
        return len(self.sequences)
    def __getitem__(self, idx):
        sequence, label = self.sequences[idx]
        
        return dict(
            sequence = torch.tensor(sequence.to_numpy(), dtype = torch.float32),
            label = torch.tensor(label, dtype = torch.float32)
        )
    
class BTCPriceDataModule(pl.LightningDataModule):
    def __init__(self, train_sequences, test_sequences, batch_size = 8):
        super().__init__()
        self.train_sequences = train_sequences
        self.test_sequences = test_sequences
        self.batch_size = batch_size
        
    def setup(self, stage = None):
        self.train_dataset = BTCDataset(self.train_sequences)
        self.test_dataset = BTCDataset(self.test_sequences)
        
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            pin_memory=True, 
            num_workers=cpu_count(),
            shuffle = False
        )
    def val_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=1,
            pin_memory=True, 
            num_workers=cpu_count(),
            shuffle = False
        )
    def predict_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=1,
            pin_memory=True, 
            num_workers=cpu_count(),
            shuffle = False
        )
```
Es importante destacar que todos los dataloader est치n paralelizados utilizando todos los cores disponibles y haciendo un `pin_memory` para aprovechar mejor la GPU. El modelo fue entrenado en [Jarvis]({{ site.baseurl}}/lightning) por lo que utiliz칩 una `RTX 3090` como GPU, la cual redujo el tiempo de entrenamiento casi 4 veces (de 8 minutos a menos de 2) y se paraleliz칩 en 16 hilos.


# Pytorch Model y Lightning Module

El modelo utilizado para este problema es una red neuronal LSTM que tendr치 9 neuronas (una para cada variable a utilizar). Contar치 con 128 celdas LSTM y 2 capas. Adem치s a modo de regularizaci칩n usarmos un dropout de un 20%.

```python
class PricePredictionModel(nn.Module):
    def __init__(self, n_features, n_hidden = 128, n_layers = 2):
        super().__init__()
        
        self.n_hidden = n_hidden
        
        self.lstm = nn.LSTM(
            input_size = n_features, 
            hidden_size = n_hidden,
            batch_first = True,
            num_layers = n_layers,
            dropout = 0.2
            
        )
        
        self.regressor = nn.Linear(n_hidden, 1)
        
    def forward(self, x):
        #self.lstm.flatten_parameters() # for distributed training
        
        _, (hidden, _) = self.lstm(x)
        #print("Shape of LSTM output: ", hidden.shape)
        out = hidden[-1]
        #print('Shape of out: ', out.shape)
        x = self.regressor(out)
        #print(x.shape)
        return x
    
class BTCPricePredictor(pl.LightningModule):
    def __init__(self, n_features: int):
        super().__init__()
        self.model = PricePredictionModel(n_features)
        self.criterion = nn.MSELoss()
        
    def forward(self, x, labels = None):
        output = self.model(x)
        loss = 0
        if labels is not None:
            loss = self.criterion(output, labels.unsqueeze(dim = 1))
            return loss, output
        return output
    
    def training_step(self, batch, batch_idx):
        sequences = batch['sequence']
        labels = batch['label']
        
        loss, output = self(sequences, labels) 
        self.log('train_loss', loss, prog_bar = True, logger = True)
        return loss
    def validation_step(self, batch, batch_idx):
        sequences = batch['sequence']
        labels = batch['label']
        
        loss, output = self(sequences, labels) 
        self.log('val_loss', loss, prog_bar = True, logger = True)

    def predict_step(self, batch, batch_idx):
        sequences = batch['sequence']
        labels = batch['label']
        
        loss, output = self(sequences, labels) 
        return labels, output
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr = 1e-4)
```

Para entrenar definimos `training_step`, `validation_step` y `predict_step` para ser consistente con los dataloaders creados anteriormente. El modelo est치 siendo optimizado mediante Adam, con un learning rate de <var>1e-4</var> usando el `MSELoss` como funci칩n de costo.

Cabe destacar que para efectos de la gr치fica que queremos construir al final del art칤culo, el `predict_step()` devolver치 tanto la etiqueta real como la etiqueta predicha (esta es una de las flexibilidades que entrega Lightning para evitar duplicaci칩n de loops).

# Entrenamiento del Modelo

Finalmente para el entrenamiento del modelo se utilizar치:

* 8 Epochs con EarlyStopping, eso quiere decir que si en 2 epochs no hay mejora en el score de validaci칩n el entrenamiento se termina.
* Usaremos un Batch Size de 64 secuencias que se ir치n pasando por la red a la vez.
* Se utilizar치 un Model Checkpoint, almacenando el mejor modelo de las epochs ejecutadas.
* Se forzar치 a que el modelo entrene al menos por 3 epochs antes de poder activar el Early Stopping.

```python
N_EPOCHS = 8
BATCH_SIZE = 64
```

```python
model = BTCPricePredictor(n_features = 9)
dm = BTCPriceDataModule(train_sequences, test_sequences, batch_size=BATCH_SIZE)
#model(item['sequence']).shape

mc = ModelCheckpoint(
    dirpath='checkpoints',
    filename = 'best-checkpoint',
    save_top_k=1,
    verbose = True,
    monitor='val_loss',
    mode = 'min'
)

early_stop = EarlyStopping(monitor = 'val_loss', patience = 2)

trainer = pl.Trainer(
    deterministic = True
    min_epochs = 3, 
    callbacks = [mc, early_stop],
    max_epochs = N_EPOCHS,
    gpus = 1,
    progress_bar_refresh_rate = 30    
)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs

```python
trainer.fit(model, dm)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name      | Type                 | Params
    ---------------------------------------------------
    0 | model     | PricePredictionModel | 203 K 
    1 | criterion | MSELoss              | 0     
    ---------------------------------------------------
    203 K     Trainable params
    0         Non-trainable params
    203 K     Total params
    0.814     Total estimated model params size (MB)

    Global seed set to 42

    Epoch 0, global step 4307: val_loss reached 0.01444 (best 0.01444), saving model to "/home/alfonso/Documents/binance/checkpoints/best-checkpoint-v1.ckpt" as top 1

    Epoch 1, global step 8615: val_loss was not in top 1

    Epoch 2, global step 12923: val_loss was not in top 1

El modelo se entren칩 por 3 epochs, en las cuales tuvo dos casos sin mejor칤a por lo que se detuvo.

```python
trained_model = BTCPricePredictor.load_from_checkpoint(
    'checkpoints/best-checkpoint-v1.ckpt',
    n_features = 9
)
```
{: title="Carga del Mejor Checkpoint"}


```python
trained_model
```

    BTCPricePredictor(
      (model): PricePredictionModel(
        (lstm): LSTM(9, 128, num_layers=2, batch_first=True, dropout=0.2)
        (regressor): Linear(in_features=128, out_features=1, bias=True)
      )
      (criterion): MSELoss()
    )


```python
trained_model.freeze()
```
{: title="Se congelan las capas para evitar que se modifiquen gradientes."}


```python
trainer.validate(model = model)
```
{: title="Inferencia de Validaci칩n"}

    --------------------------------------------------------------------------------
    DATALOADER:0 VALIDATE RESULTS
    {'val_loss': 0.014435895718634129}
    --------------------------------------------------------------------------------

## Verificaci칩n en datos de validaci칩n

```python
preds = trainer.predict(model = trained_model, dataloaders = dm.val_dataloader())
```

```python
labels = torch.tensor(preds)[:,0]
predictions = torch.tensor(preds)[:,1]
```

Es importante destacar ac치 que devolvimos etiquetas y labels. Transformando el output en tensor es posible obtener dos tensores independientes de cada set de etiquetas.


## Creaci칩n de un Descaler 

Es importante recordar que para el proceso de entrenamiento toda la data fue escalada. Por lo tanto para ver efectivamente nuestro resultado se necesita traerla a la escala real. Para ello se hace el siguiente truco para poder aprovechar el `inverse_transform` de `Scikit-Learn`.

```python
descaler = MinMaxScaler()
descaler.min_, descaler.scale_ = scaler.min_[-1], scaler.scale_[-1]
```
{% include alert todo='`scaler.min_` y `scaler.scale_` son los par치metros aprendidos por MinMaxScaler. Basicamente ac치 estamos simulando un entrenamiento artificial utilizando s칩lo los par치metros de nuestro label.'%}

```python
def descale(values, descaler = descaler):
    values_2d = np.array(values)[:, np.newaxis]
    return descaler.inverse_transform(values_2d).flatten()
```

```python
predictions_descaled = descale(predictions)
labels_descaled = descale(labels)
```
{: title="Se traen los valores a escala real."}


```python
test_data = df[train_size + 1:]
len(test_data), len(test_df)
```

    (30643, 30643)

Luego verificamos que los datos de test sean consistentes con lo que generamos, ambos deben entregarnos el mismo n칰mero de registros.

Finalmente, tenemos que notar que no es posible generar predicciones para las secuencias menores al largo de Secuencia ya que no cuentan con secuencias necesarias para predecir. Por lo tanto, generamos el siguiente set de secuencias para luego extraer la fecha:

```python
test_sequences_data = test_data.iloc[SEQUENCE_LENGTH:]
len(test_sequences_data), len(test_sequences)
```
    (30523, 30523)

```python
test_sequences_data.head()
```

<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>unix</th>
      <th>date</th>
      <th>symbol</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>Volume BTC</th>
      <th>Volume USDT</th>
      <th>tradecount</th>
      <th>prev_close</th>
      <th>close_change</th>
      <th>day_of_week</th>
      <th>day_of_month</th>
      <th>week_of_year</th>
      <th>month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>967127</th>
      <td>1626294180000</td>
      <td>2021-07-14 20:23:00</td>
      <td>BTC/USDT</td>
      <td>32794.15</td>
      <td>32800.00</td>
      <td>32772.74</td>
      <td>32779.19</td>
      <td>6.164449</td>
      <td>202108.117635</td>
      <td>278</td>
      <td>32794.15</td>
      <td>-14.96</td>
      <td>2</td>
      <td>14</td>
      <td>28</td>
      <td>7</td>
    </tr>
    <tr>
      <th>967128</th>
      <td>1626294240000</td>
      <td>2021-07-14 20:24:00</td>
      <td>BTC/USDT</td>
      <td>32780.65</td>
      <td>32808.76</td>
      <td>32780.64</td>
      <td>32804.94</td>
      <td>24.302589</td>
      <td>797120.586022</td>
      <td>388</td>
      <td>32779.19</td>
      <td>25.75</td>
      <td>2</td>
      <td>14</td>
      <td>28</td>
      <td>7</td>
    </tr>
    <tr>
      <th>967129</th>
      <td>1626294300000</td>
      <td>2021-07-14 20:25:00</td>
      <td>BTC/USDT</td>
      <td>32804.93</td>
      <td>32823.71</td>
      <td>32804.93</td>
      <td>32817.51</td>
      <td>10.519932</td>
      <td>345226.325132</td>
      <td>359</td>
      <td>32804.94</td>
      <td>12.57</td>
      <td>2</td>
      <td>14</td>
      <td>28</td>
      <td>7</td>
    </tr>
    <tr>
      <th>967130</th>
      <td>1626294360000</td>
      <td>2021-07-14 20:26:00</td>
      <td>BTC/USDT</td>
      <td>32814.92</td>
      <td>32823.42</td>
      <td>32812.00</td>
      <td>32820.00</td>
      <td>5.496652</td>
      <td>180388.987189</td>
      <td>284</td>
      <td>32817.51</td>
      <td>2.49</td>
      <td>2</td>
      <td>14</td>
      <td>28</td>
      <td>7</td>
    </tr>
    <tr>
      <th>967131</th>
      <td>1626294420000</td>
      <td>2021-07-14 20:27:00</td>
      <td>BTC/USDT</td>
      <td>32819.99</td>
      <td>32843.55</td>
      <td>32819.96</td>
      <td>32821.31</td>
      <td>11.457531</td>
      <td>376199.073946</td>
      <td>351</td>
      <td>32820.00</td>
      <td>1.31</td>
      <td>2</td>
      <td>14</td>
      <td>28</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>

Finalmente graficamos los valores reales del periodo de Test y los obtenidos por nuestra predicci칩n:

```python
dates = matplotlib.dates.date2num(test_sequences_data.date.tolist())
plt.figure(figsize = (10,6))
plt.plot_date(dates, predictions_descaled, '-', label = 'predicted')
plt.plot_date(dates, labels_descaled, '-', label = 'real')
plt.xticks(rotation = 45)
plt.title('Predicci칩n BitCoin 2021')
plt.legend();
```

![picture of me]({{ site.urlimg }}bitcoin/output_47_0.png){: .center }

## Disclaimer

No soy experto en temas de Finanzas ni Criptomonedas por lo que si se quiere aplicar modelos como estos para sus inversiones aseg칰rese de asesorarse. No creo que sea un modelo malo, pero si tengo las siguientes consideraciones:

* Tanto el entrenamiento como la validaci칩n est치 medido en datos del pasado, y funciona bien. Pero este modelo no puede predecir que Elon Musk de alg칰n comunicado y el precio se del BitCoin se caiga de un d칤a para otro.
* Si bien logra predecir de muy buena manera las tendencias, no predice la magnitud correcta. 
* Probablemente entrenando por m치s epochs y/o utilizando m치s data puede ayudar.
* Quiz치s una buena idea sea eliminar el Early Stopping y dejar que el modelo entrene las 8 epochs completas.
* Un mejor afinamiento de hiperpar치metros puede ser de gran utilidad.

Espero que este tutorial haya sido de ayuda y se pueda entender el gran potencial que se tiene al utilizar redes neuronales.

## UPDATE TL;DR

Luego de subir este post recib칤 algunos comentarios acerca de la metodolog칤a utilizada. Quiero agradecer particularmente a [Mario Leni](https://www.linkedin.com/in/mario-jos%C3%A9-leni-rodr%C3%ADguez-baa616150/), quien me comparti칩 el siguiente video en el cual se critican algunas de las pr치cticas utilizadas para la predicci칩n de Stocks y me gustar칤a hacer algunos comentarios al respecto:

<div class='embed-youtube'>
{% include youtubePlayer.html id="Vfx1L2jh2Ng" %}
</div>

<br>

* Primero, el video est치 subido por un tipo que se denomina LazyProgrammer. He tenido la oportunidad algunos cursos de 칠l en Udemy y la verdad es que explica super bien. En su sitio web menciona que tiene 2 Masters:
  * Uno en Computer engineering con Especializaci칩n en Machine Learning y Reconocimiento de Patrones (largo el nombre)
  * Y uno en estad칤sticas.
Que el tipo tiene credenciales tiene...

Ahora, es muy extra침o como parte su video porque no me gusta mucho su tono. Tiene un tono agradable de voz pero igual usa palabras que encuentro pesadas. Es raro porque siento que su video es medio *pataleta* (como un berrinche) porque la gente est치 equivocada y reclama varias cosas sin una estructura l칩gica (pero igual hace propaganda a sus cursos):
  * Que nadie escribe su c칩digo.
  * Que uno copia y pega arrastrando errores.
  * Que est치 mal utilizar el `MinMaxScaler` (algo que yo hice en mi implementaci칩n y se supone es su reclamo principal).
  * Que no se debe usar el precio, sino que los returns.

### Que nadie escribe su c칩digo

Eso es cierto, una de las ventajas del c칩digo es poder reutilizarlo, y poder reproducir algo que alguien m치s hizo sin que tenga que demostrar c칩mo lo hizo. En mi caso yo s칤 tom칠 el c칩digo de varios otros posteos, pero no estaba en Pytorch Lightning. De hecho, yo lo adapt칠 y sufr칤 harto para hacerlo funcionar, y entrenar el modelo cost칩, por eso la diferencia entre la data que saqu칠 hasta que lo publiqu칠.

### Que uno copia y pega arrastrando errores

Es posible, en mi caso, siempre me cuestiono lo que hago con mi c칩digo para entenderlo. No suelo comprar lo que todos dicen. De hecho, especialmente en el mundo Pytorch hay mucho c칩digo boilerplate que uno solamente copia y pega sin saber para qu칠 sirve. Yo no puedo con eso, me quita el sue침o colocar c칩digo que no entiendo y siempre trato de estar super conciente de por qu칠 se aplica cada parte. Una cr칤tica que 칠l reclamaba en particular es el hecho de usar secuencia de largo 1. Es muy probable que sea un error, y que mucha gente no se lo cuestione (b치sicamente no es una secuencia). En mi caso us칠 secuencias de 120. 

Es interesante que en un punto dice que no se puede predecir stocks como secuencias, porque son aleatorios (~2:40). Creo que esa es precisamente la idea de las redes recurrentes: ver si es que hay alg칰n patr칩n dif칤cil de encontrar al ojo humano y aprender de 칠l. Pero aprender no significa repetir, el punto que 칠l menciona me da la impresi칩n que se refiere a redes que hacen overfitting, toman el patr칩n y lo repiten por que no saben generalizar. Si fuera porque nada realmente es una secuencia porque es aleatorio, no podr칤amos predecir nada.

### Que est치 mal utilizar el `MinMaxScaler`

Es raro, reclama todo el rato que est치 mal hacerlo pero al final dice que no est치 *tan* mal (~10:00). Su punto es que a diferencia de las ondas, el stock price no est치 acotado por lo tanto no puedo asumir el m치ximo como 1 y el m칤nimo como -1. Yo personalmente creo que esto est치 mal. El `MinMaxScaler` s칩lo mueve la escala, en ning칰n momento obliga a la red neuronal a predecir en el rango -1 a 1. De hecho, la implementaci칩n tiene como capa de salida una red densamente conectada sin funci칩n de activaci칩n (esto por ser un problema de regresi칩n). Por lo tanto, al no tener una funci칩n sigmoide no est치 forzada a predecir en este rango. 

Entiendo que las redes neuronales no se caractarizan por su capacidad de extrapolar, y aqu칤 puede ser que impl칤citamente estamos colocando un techo a la red. Pero de nuevo el objetivo de este tipo de post es mostrar c칩mo se entrena un modelo de serie de tiempos, no volverse rico invirtiendo en BitCoin.

### Que no se debe usar el precio, sino que los returns

Este punto no lo entiendo. Si me interesa investigar el precio, tratar de entender hasta cuanto va a llegar el precio, 쯣or qu칠 deber칤a usar el return? Su raz칩n es que no tiene tendencia. Bueno si la tendencia es el problema de la serie de tiempo de Precios, se puede quitar la tendencia aplicando diferenciaci칩n. Es muy sencillo de aplicar y me permite no tener que cambiar la variable que quiero monitorear. Probablemente si quiero ganar plata el return es una mejor opci칩n, pero a칰n as칤 no me calza la cr칤tica.

Yo creo que est치 bien criticar cuando uno ve un proceso incorrecto. Pero en particular en el uso de redes neuronales, donde ni los Kaggle GrandMasters est치n 100% seguros de lo que hacen, no me parece la manera. El ingl칠s no es mi idioma nativo, pero estoy casi seguro que <q>idiotic</q> (~3:00) no es una palabra buena onda. Y la conclusi칩n que saco es que hace una cr칤tica s칩lo para vender sus cursos.

Creo que una de las caracter칤sticas que tienen que primar en este campo es la humildad. He visto c칩digo horriblemente malo de Phds, Kaggle Grandmasters, incluso he visto errores de concepto por parte de LazyProgrammer (tom칠 uno de sus cursos y aplicaba un Softmax a la salida de un problema multiclase utilizando como Loss Function CrossEntropy Loss, un concepto incorrecto y que se desaconseja/desaprueba en los foros de Pytorch), pero este es un campo en desarrollo, nadie sabe realmente c칩mo funcionan las Redes Neuronales. Y usando su mismo argumento, si 칠l s칤 sabe como hacer Stock Prediction, por qu칠 no es millonario invirtiendo en BitCoin (o quiz치s ya lo es 游뱂).

Sorry por lo latero, 

Nos vemos la pr칩xima,

[**Alfonso**]({{ site.baseurl }}/contact/)

*[Binance]: Plataforma de Intercambio de Criptomonedas.