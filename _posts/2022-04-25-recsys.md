---
permalink: /recsys/ 
title: "¬øC√≥mo funciona un Sistema Recomendador?"
subheadline: "Creando un Recomendador de Pel√≠culas"
teaser: "Neural Collaborative Filtering (NCF) en Pytorch"
# layout: page-fullwidth
usemathjax: true
category: pp
header: no
image:
    thumb: recsys/front.png
tags:
- pytorch
- tutorial
- dl
published: true
---

![picture of me]({{ site.urlimg }}recsys/front.png){: .left .show-for-large-up .hide-for-print width="250"}
![picture of me]({{ site.urlimg }}recsys/front.png){: .center .hide-for-large-up width="500"}

Los sistemas de Recomendaci√≥n est√°n en todas partes. Abrimos Netflix, Amazon, Spotify, sin ir m√°s lejos el mismo Mercado Libre, y todos siempre tienen algo que ofrecernos. Los sistemas de recomendaci√≥n son otro tipo de modelos de modelos de Machine Learning, y son el eje principal en las grandes de Silicon Valley. <!--more--> Pero, conversando con [Gustavo Prudencio](https://www.linkedin.com/in/gustavo-prudencio-cerfogli/) de Cornershop, ambos estuvimos de acuerdo en que no son para nada Modelos Populares. Es m√°s, particularmente en Chile, creo que es de los modelos m√°s raros de ver. Probablemente Cornershop sea de los pioneros en esto (aunque no me quiso dar detalles de qu√© tienen actualmente implementado üòÜ).

En fin, no soy experto en modelos de recomendaci√≥n. Pero por ah√≠ en el 2019 me toc√≥ armar un piloto en Cencosud Scotiabank utilizando filtrado colaborativo con librer√≠as como implicit, surprise y lightfm pero nunca llegu√© a entenderlo a profundidad. Este fin de semana decid√≠ aprender en m√°s profundidad de esto e implementar un modelo de Recomendaci√≥n en Pytorch (as√≠ como si tuviera tiempo de sobra, el cual no tengo).

## Modelo de Recomendaci√≥n

Entender el problema de recomendaci√≥n es muy sencillo, hay dos tipos de recomendadores, los que usan rating expl√≠cito y rating impl√≠cito.

* **Rating Expl√≠cito**: Es cuando un usuario de manera expl√≠cita califica un producto: Notas, estrellas, lo que sea. El tema con este tipo de data es que es rara porque normalmente no es obligatorio calificar un producto.

* **Rating Impl√≠cito**: Es cuando la calificaci√≥n del producto se da de manera impl√≠cita. Normalmente se puede dar como: compra o no compra un producto, ve o no ve un video, escucha o no una canci√≥n, etc.

El objetivo del modelo de Recomendaci√≥n determinar qu√© producto ser√≠a bueno mostrarle al usuario. Para ello existen distintos approaches. El m√°s com√∫n hoy el d√≠a es el filtrado colaborativo. Es una t√©cnica que consiste en que se recomendar√°n productos que usuarios parecidos a ti hayan visto. Por lo tanto, no influye solo lo que t√∫ has visto, sino que tambi√©n lo que gente con gustos similares a los tuyos han visto.

Mi inter√©s es poder implementar un modelo de Deep Learning que tenga esto en cosideraci√≥n, por lo tanto, decid√≠ utilizar el siguiente [paper](https://arxiv.org/abs/1708.05031) implementando un Neural Collaborative Filtering:

![picture of me]({{ site.urlimg }}recsys/ncf.png){: .center }

Este modelo est√° basado en Embeddings, que es una especie de Encoding en el cual se representa un espacio de alta dimensionalidad en un espacio de menor dimensionalidad en el cual la distancia de las representaciones pueden tener una cierta interpretaci√≥n. Esto es particularmente importante porque normalmente los sistemas recomendadores est√°n implementados cuando hay muchos usuarios y muchos productos.

Para el caso que quiero mostrar voy a utilizar un dataset llamado `MovieLens 25M`, el cual se puede descargar de [ac√°](https://grouplens.org/datasets/movielens/25m/). La raz√≥n por la que escog√≠ este dataset es porque contiene una lista de usuarios y pel√≠culas calificadas por usuarios hasta el 2019, que incluye muchas pel√≠culas actuales (aunque pre-pandemia). El problema del dataset es que contiene 25 millones de ratings, 62423 pel√≠culas y 162541 usuarios. 

{% include alert alert='No voy a utilizar el dataset completo porque no quiero ensuciar las recomendaciones con pel√≠culas antiguas, pero de igual manera quiero como desaf√≠o personal trabajar con una gran cantidad de datos. Es en este tipo de problemas cuando realmente es necesario tener buenas skills de programaci√≥n para poder lidiar con alta cantidad de datos.'%}

{% include alert info='Esto no es Big Data, es harta data pero para que se vea el poder del Stack de Data Science no vamos a usar nada extra√±o, s√≥lo Pandas, Scipy y Numpy.'%}

Entonces para poder entender qu√© hace el modelo encontr√© el siguiente ejemplo:
Supongamos que Bob no es muy fan de las peliculas de Romance, pero s√≠ de ls pel√≠culas de Acci√≥n, mientras que Joe, le gustan ambas. Podemos dependiendo del id de usuario y de las id de las pel√≠culas vistas transformarlo en lo siguiente:

![picture of me]({{ site.urlimg }}recsys/embeddings.png){: .center }

Luego su representaci√≥n en el plano bidimensional Action x Romance nos permite identificar qu√© tan parecidos o distintos son Bob y Joe. Esto permitir√° al modelo aprender las relaciones que existen entre usuarios al momento de poder recomendar.

![picture of me]({{ site.urlimg }}recsys/distance.png){: .center }

Teniendo estas ideas b√°sicas del funcionamiento de un recomendador vamos a la implementaci√≥n:

## Entendiendo los datos

### Pel√≠culas

```python
import pandas as pd
import numpy as np
```
{: title="Importaci√≥n Librer√≠as."}


```python
movies = pd.read_csv('ml-25m/movies.csv')
print(movies.shape)
```

    (62423, 3)

    Index(['movieId', 'title', 'genres'], dtype='object')

El set de pel√≠culas contiene s√≥lo el Id de Pel√≠cula, el t√≠tulo y los g√©neros asociados a cada pel√≠cula. La verdad es que el formato no es mi favorito, por lo que decid√≠ limpiar un poco la data para por ejemplo poder obtener el a√±o de cada pel√≠cula. Adem√°s como dijimos anteriormente, no queremos recomendar pel√≠culas viejas por lo que decid√≠ mantener s√≥lo las pel√≠culas de 2010 en adelante.



```python
year = 2010
movies['year'] = movies.title.str.extract(r'\((\d{4})\)').astype("float")
movie_id_removed = movies.query('year < @year').movieId.tolist()
movies = movies.query('year >= @year')
movies
```

<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>movieId</th>
      <th>title</th>
      <th>genres</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14156</th>
      <td>73268</td>
      <td>Daybreakers (2010)</td>
      <td>Action|Drama|Horror|Thriller</td>
      <td>2010.0</td>
    </tr>
    <tr>
      <th>14161</th>
      <td>73319</td>
      <td>Leap Year (2010)</td>
      <td>Comedy|Romance</td>
      <td>2010.0</td>
    </tr>
    <tr>
      <th>14162</th>
      <td>73321</td>
      <td>Book of Eli, The (2010)</td>
      <td>Action|Adventure|Drama</td>
      <td>2010.0</td>
    </tr>
    <tr>
      <th>14222</th>
      <td>73744</td>
      <td>If You Love (Jos rakastat) (2010)</td>
      <td>Drama|Musical|Romance</td>
      <td>2010.0</td>
    </tr>
    <tr>
      <th>14256</th>
      <td>73929</td>
      <td>Legion (2010)</td>
      <td>Action|Fantasy|Horror|Thriller</td>
      <td>2010.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>62412</th>
      <td>209143</td>
      <td>The Painting (2019)</td>
      <td>Animation|Documentary</td>
      <td>2019.0</td>
    </tr>
    <tr>
      <th>62413</th>
      <td>209145</td>
      <td>Libert√© (2019)</td>
      <td>Drama</td>
      <td>2019.0</td>
    </tr>
    <tr>
      <th>62415</th>
      <td>209151</td>
      <td>Mao Zedong 1949 (2019)</td>
      <td>(no genres listed)</td>
      <td>2019.0</td>
    </tr>
    <tr>
      <th>62418</th>
      <td>209157</td>
      <td>We (2018)</td>
      <td>Drama</td>
      <td>2018.0</td>
    </tr>
    <tr>
      <th>62420</th>
      <td>209163</td>
      <td>Bad Poems (2018)</td>
      <td>Comedy|Drama</td>
      <td>2018.0</td>
    </tr>
  </tbody>
</table>
<p>20489 rows √ó 4 columns</p>
</div>

Esto nos dej√≥ con 20489 pel√≠culas pero que no deja de ser un n√∫mero considerable.


Adem√°s es importante guardar los id de las pel√≠culas removidas que fueron 41524. Estas tienen que retirarse tambi√©n de los reviews de los usuarios.
```python
len(movie_id_removed)
```

    41524

Finalmente, cre√© un mapping entre el movieId y el nombre de la pel√≠cula. Esto ser√° de gran utilidad al final del procedimiento, para poder identificar los movieId recomendado para ver si es que hacen sentido.

```python
movies_mapping = movies[['movieId','title']].set_index('movieId').to_dict()['title']
```
{: title="Mapeo de Pel√≠culas."}


# Calificaciones

Por otro lado, tenemos el dataset de Ratings, el cual contiene los 25 millones de datos. Estos contienen los distintos usuarios con las pel√≠culas vistas y sus reviews. Adem√°s incluye un `timestamp` para poder por ejemplo, tener informaci√≥n de cuando vi√≥ la pel√≠cula, en caso de que el orden tambi√©n tenga relevancia para la recomendaci√≥n.

```python
ratings = pd.read_csv('ml-25m/ratings.csv', parse_dates=['timestamp'])
print(ratings.columns)
print(ratings.shape)
ratings.userId.nunique()

```
    Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')
    (25000095, 4)

    162541

Entonces, dentro del procesamiento de los datos tenemos que primero eliminar todas las pel√≠culas anteriores a 2010 (por eso guardamos los movieId de las pel√≠culas). Adem√°s modificaremos el rating a 1. Esto convertir√° nuestro problema en un recomendador impl√≠cito. Es decir, el 1 significar√° que el usuario interactu√≥ con la pel√≠cula, es decir, la vio. Esto es importante para el algoritmo de recomendaci√≥n ya que entonces tendremos que modelar nuestro problema como un problema de clasificaci√≥n.

```python
ratings = ratings.query('movieId not in @movie_id_removed')
ratings['rating'] = 1
ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
ratings
```

<div class='table-overflow'>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>712</th>
      <td>3</td>
      <td>73268</td>
      <td>1</td>
      <td>2015-08-13 14:11:38</td>
    </tr>
    <tr>
      <th>713</th>
      <td>3</td>
      <td>73321</td>
      <td>1</td>
      <td>2015-08-13 13:52:05</td>
    </tr>
    <tr>
      <th>715</th>
      <td>3</td>
      <td>74458</td>
      <td>1</td>
      <td>2017-04-21 14:39:18</td>
    </tr>
    <tr>
      <th>716</th>
      <td>3</td>
      <td>74789</td>
      <td>1</td>
      <td>2019-08-18 00:59:42</td>
    </tr>
    <tr>
      <th>717</th>
      <td>3</td>
      <td>76077</td>
      <td>1</td>
      <td>2017-01-18 16:15:09</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>24999773</th>
      <td>162538</td>
      <td>111617</td>
      <td>1</td>
      <td>2015-08-05 14:15:09</td>
    </tr>
    <tr>
      <th>24999774</th>
      <td>162538</td>
      <td>112138</td>
      <td>1</td>
      <td>2015-08-05 14:14:35</td>
    </tr>
    <tr>
      <th>24999775</th>
      <td>162538</td>
      <td>112556</td>
      <td>1</td>
      <td>2015-08-05 14:25:33</td>
    </tr>
    <tr>
      <th>24999776</th>
      <td>162538</td>
      <td>116797</td>
      <td>1</td>
      <td>2015-08-05 13:25:21</td>
    </tr>
    <tr>
      <th>24999777</th>
      <td>162538</td>
      <td>126548</td>
      <td>1</td>
      <td>2015-08-05 14:24:57</td>
    </tr>
  </tbody>
</table>
<p>2711937 rows √ó 4 columns</p>
</div>

{% include alert tip='Este es quiz√°s uno de los proyectos que m√°s he disfrutado haciendo, y la raz√≥n principal es porque el Proceso de un Motor de Recomendaci√≥n es bastante m√°s complejo que s√≥lo entrenar el modelo. La data tiene que ser manipulada de muchas maneras distintas (incluyendo sus complejidades por el tama√±o, as√≠ que veamos c√≥mo me las ingeni√©)'%}

### Label Encoder

Debido a los recortes que hicimos en la data, nuetros Id, tanto de Usuarios como de pel√≠culas no tienen porque ser consecutivos. Esto nos pueden traer alg√∫n problema para el algoritmo ya que el id va a representar una distancia en nuestro espacio de embeddings y no queremos que esto se vea alterado. Para ello entonces utilizaremos el `LabelEncoder` para crear un mapeo entre los ids reales y un id correlativo. 

{% include alert info='Esto es b√°sicamente lo mismo que hicimos con el mapeo de pel√≠culas, pero eventualmente al momento de produccionalizar esto tendremos que tener acceso r√°pido a nuestros mapeos, por lo que el `LabelEncoder` permite una f√°cil serializaci√≥n de ellos, ya que las clases quedar√°n como listas.'%}

```python
from sklearn.preprocessing import LabelEncoder

user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()
ratings['userId'] = user_encoder.fit_transform(ratings.userId)
ratings['movieId'] = movie_encoder.fit_transform(ratings.movieId)
```

### Primer Tropez√≥n

Dado que este es un recomendador impl√≠cito, va a ser modelado como un problema de clasificaci√≥n binaria. Por lo que tenemos muchas pel√≠culas que el usuario ha visto, pero no tenemos las que no ha visto. Para que el modelo pueda aprender bien le√≠ que una buena idea es poder entregar casos negativos, es decir, pel√≠culas que no ha visto. Y un buen ratio era 4:1, es decir 4 pel√≠culas no vistas por cada pel√≠cula vista.

Mi primer approach fue este:

```python
def create_negative_movies(df, userid = 'userId', movieid = 'movieId',neg_examples = 4):
    unique_movies = set(df[movieid])
    
    movies = []
    uids = df[userid].unique()
    for u in uids:
        movies.extend(np.random.choice(list(unique_movies - set(df[movieid][df[userid] == u])), size = neg_examples))
        
    return uids, movies
```

![picture of me]({{ site.urlimg }}recsys/mala_imp.png){: .center }

Intent√© una implementaci√≥n, que de partida estaba mala, pero que demor√≥ 20 minutos en ejecutarse. 
B√°sicamente el c√≥digo hace lo siguiente:

* Para cada usuario en la lista de usuarios.
* Calculo la diferencia entre todos los ids de pel√≠culas y las que ha visto un usuario.
* A partir de las pel√≠culas no vistas saco un random de 4 ejemplos.

Este approach est√° incorrecto porque gener√© s√≥lo 4 ejemplos por usuario, unos 400K registros extras y demor√≥ demasiado debido a los muchos usuarios, y debido a que filtrar un dataset tan grande tantas veces lo hace muy lento.

### La soluci√≥n

Si han estudiado el mecanismo de atenci√≥n de los transformers, notar√°n que b√°sicamente se basan en un one-hot encoder para utilizarla como una matriz de filtrado (si quieren estudiar esto en detalle pueden leerlo [ac√°](https://e2eml.school/transformers.html#one_hot)). Con esto podr√≠amos crear una matriz cuyas filas sean los ids de los usuarios y las columnas los ids de las pel√≠culas. Esto se conoce como una `user-item matrix` y basta con hacerla as√≠:

```python
user_item_matrix = ratings.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)
```
El problema es que esta matriz es gigantesca y me dio el cl√°sico error `Unable to allocate 1010. MiB for an array with shape...`.

Entonces, aqu√≠ es donde hay que ponerse creativo. B√°sicamente la `user-item matrix` es una matriz rala (escasa, esparsa, llena de ceros, no s√© cu√°l es la terminolog√≠a correcta, sparse matrix en ingl√©s...). Y `scipy` tiene matrices especiales para eso. `csr_matrix` s√≥lo almacena los √≠ndices de los valores distintos de cero. Este tipo de matriz es extremadamente eficiente para sumas y productos matriciales, los cuales no vamos a usar, pero s√≠ queremos beneficiarnos de la eficiencia.

```python
from scipy.sparse import csr_matrix
np.random.seed(42)
def create_matrix(data, user_col, item_col, rating_col):

    data[[user_col, item_col]] = data[[user_col, item_col]].astype('category')
    
    rows = data[user_col].cat.codes
    cols = data[item_col].cat.codes
    rating = data[rating_col]
    user_item_matrix = csr_matrix((rating, (rows, cols)))
    return user_item_matrix

user_item_matrix = create_matrix(ratings, 'userId', 'movieId', 'rating')
```

Encontr√© una implementaci√≥n que b√°sicamente toma los √≠ndices de usuarios y pel√≠culas y en la cordenada (userId, movieId) rellena el 1 o cero si es que dicho usuario vio o no la pel√≠cula. Crear esto toma exactamente nada:

![picture of me]({{ site.urlimg }}recsys/uim.png){: .center }

Antes eso s√≠ de generar las clases negativas decid√≠ generar el split de la data. Esto con el fin de poder evaluar el comportamiento del modelo:

{% include alert tip='La verdad es que en mi proceso real hice el split antes, pero luego me di cuenta que era mejor crear la `user-item matrix` primero. Esto porque si creaba este procedimiento despu√©s tendr√≠a que crear dos `user-item matrix`, una para el train_set y otra para el test_set. El gran problema de esto, es que los √≠ndices en test son pel√≠culas que no est√°n en train, por lo que iba a tener problemas para identificar el id correcto ya que el `test_set` iba a tener dimensiones distintas y el id 0 de test no iba a corresponder a la pel√≠cula con id cero, si no a la primera pel√≠cula del `test_set` que podr√≠a ser una pel√≠cula arbitraria. Esto me forzar√≠a a hacer otro mapeo, el cual no quise hacer. Todo ese problema me lo evit√© haciendo el split despu√©s.'%}

### Train-Test Split

Para poder generar el split sin generar leakage se recomendaba utilizar el siguiente procedimiento:

```python
ratings['test'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)

train_ratings = ratings.query('test != 1').drop(columns = ['test', 'timestamp'])
test_ratings = ratings.query('test == 1').drop(columns = ['test', 'timestamp'])
```

B√°sicamente estamos aprendiendo de todas las pel√≠culas excepto la √∫ltima que cada usuario vi√≥. La √∫ltima pel√≠cula estar√° en el `test_set`, por lo que esperamos que nuestro recomendador efectivamente pueda recomendar la √∫ltima pel√≠cula que vi√≥.

## Generaci√≥n de Pel√≠culas no vistas

Ahora gener√© otra implementaci√≥n utilizando la `user-item matrix`. Debido a la fea sint√°xis de `numpy`, el resultado dejaba muchas funciones anidadas, por lo que decid√≠ escribirlo estilo `Pytorch`:

```python
def create_negative_df(user_ids, user_item, neg_examples = 4, test = False):
    
    movies_id = np.arange(user_item.shape[1])
    negative_movies = []
    examples = []
    for i in range(len(user_ids)):

        interacted = user_item[i].nonzero()[1]
        x = ~np.isin(movies_id, interacted)
        x = np.argwhere(x).squeeze(1)
        
        if test:
            size = neg_examples
        else:
            size = len(interacted)*neg_examples
        
        x = np.random.choice(x, size = size)
        negative_movies.extend(x)
        examples.append(size)
        
    negative_movies_df = pd.DataFrame(dict(userId = np.repeat(user_ids, examples),
                        movieId = negative_movies,
                        rating = np.zeros(len(negative_movies)))
                        )
    return negative_movies_df
```

Esta funci√≥n:
* Toma los ids de pel√≠culas y para cada id de usuario detecta las pel√≠culas con que interactu√≥.
* Genera un mask de pel√≠culas no vistas, es decir, donde la user-item matrix no es uno.
* Calcula qu√© indices son las pel√≠culas no vistas.
* Saca una muestra de toda las pel√≠culas no vistas igual a `neg_examples` en el caso de test y de `neg_examples` por el n√∫mero de pel√≠culas vistas en otro caso.
* Finalmente combina todo eso en un DataFrame (con una l√≥gica bien enredada que no s√© muy bien como explicar as√≠ que pueden deducirlo del c√≥digo).

> Esta funci√≥n la implement√© solito (sin StackOverflow).

Esta funci√≥n entonces sacar√° 4 ejemplos no vistos por cada pel√≠cula vista por un usuario en train. En el caso de test es un poco distinto, dado que cada usuario tiene s√≥lo una pel√≠cula vista, sacaremos 99 casos no vistos (aleatorios). Luego la predicci√≥n del modelo ser√° las 10 recomendaciones de los 100 casos, donde una de ellas ha sido vista por el usuario. Esperamos que en las 10 recomendaciones se encuentre la pel√≠cula que efectivamente vio.

![picture of me]({{ site.urlimg }}recsys/split.png){: .center }

Como pueden ver esta implementaci√≥n es muy r√°pida y eficiente y los resultados son 10 millones de registros para el train y 6 para el test.

Finalmente se combinan los casos vistos y no vistos obteniendo 12 millones para el `train set` y 6 millones para el `test set`.

```python
full_training_df = train_ratings.append(train_negative_movies_df)
full_test_df = test_ratings.append(test_negative_movies_df)

full_training_df.shape, full_test_df.shape
```
    ((12797849, 3), (6078000, 3))


# Neural Collaborative Filtering

Como siempre el modelo lo crearemos utilizando Pytorch Lightning. Justo actualic√© Pytorch Lightning a la versi√≥n 1.6.1 el cu√°l trajo varios cambios en la API por lo que aprovechar√© de mencionar dichos cambios:


```python
import torch
import torch.nn as nn
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint

pl.seed_everything(42, workers=True)
```
    Global seed set to 42

Primero que todo fijamos la semilla para la reproducibilidad. Adem√°s, encontr√© que utilizando `workers=True` se garantiza la reproducibilidad en los DataLoaders, que a veces no eran tan f√°ciles de reproducir debido a la carga de la data en GPU.


```python
from torch.utils.data import Dataset, DataLoader

class MovieData(Dataset):
    def __init__(self, users, movies, ratings):
        self.users = users
        self.movies = movies
        self.ratings = ratings
        
    def __len__(self):
        return len(self.ratings)
        
    def __getitem__(self, idx):
    
        users = self.users.iloc[idx]
        movies = self.movies.iloc[idx]
        ratings = self.ratings.iloc[idx]

        return dict(
            users = torch.tensor(users, dtype=torch.long),
            movies = torch.tensor(movies, dtype=torch.long),
            ratings = torch.tensor(ratings, dtype=torch.float)
        )
```

Creamos nuestro Pytorch `Dataset`, que b√°sicamente tomar√° los usuarios, pel√≠culas y ratings y los transformar√° en tensores. 

```python
class MovieDataModule(pl.LightningDataModule):
    def __init__(self, train_df, test_df, batch_size = 512):
        super().__init__()
        
        self.train_df = train_df 
        self.test_df = test_df 
        self.batch_size = batch_size
        
    def setup(self, stage=None):
        
        self.train_data = MovieData(self.train_df.userId, self.train_df.movieId, self.train_df.rating)
        self.test_data = MovieData(self.test_df.userId, self.test_df.movieId, self.test_df.rating)
    
    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, pin_memory=True, num_workers = 10)
    
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, pin_memory=True, num_workers = 10)
```
En este caso el `LightningDataModule` tomar√° los set de train y test los transformar√° en tensores y los cargar√° en GPU con los DataLoader. Si se fijan el `batch_size` lo dej√© en 512 porque es mucha data y batch size peque√±os demoraban demasiado. El tema de usar batch_size alto es que hizo explotar mis DataLoaders muchas veces, y la raz√≥n de eso es porque ten√≠a mi `num_workers` en 12 (que son todos mis core en mi laptop). Luego de mucho batallar, encontr√© que era mejor decisi√≥n bajar esto y dejar algunos libres. Hay que recordar que la funci√≥n del DataLoader es cargar la data al modelo y en este caso hacer el traspaso a la GPU, pero este proceso se realiza en CPU, por lo que es bueno dejar unos cores para que el compu pueda sobrevivir el proceso de entrenamiento.

```python
class NCF(nn.Module):
    def __init__(self, dim_users, dim_movies, n_out = 1):
        super().__init__()
        
        self.user_embedding = nn.Embedding(dim_users, 8)
        self.movie_embedding = nn.Embedding(dim_movies, 8)
        
        self.encoder = nn.Sequential(
                            nn.Linear(16,64),
                            nn.ReLU(inplace=True),
                            nn.Linear(64,32),
                            nn.ReLU(inplace=True),
                            nn.Linear(32,n_out)
                        )
        
    def forward(self, users, movies):
        user_emb = self.user_embedding(users)
        movie_emb = self.movie_embedding(movies)
        
        x = torch.cat((user_emb, movie_emb), dim = 1)
        x = self.encoder(x)
        return x
```
El modelo de Neural Collaborative Filtering es una red neuronal que toma como entrada los usuarios y pel√≠culas y devuelve un rating. En este caso el modelo es una red neuronal que parte con un embedding tanto para users como movies de 8 dimensiones, que se concatenan para entrar en un encoder compuesto por una capa de 64 dimensiones, una capa de 32 dimensiones y una capa de 1 dimension (el rating). (Pueden creer que el parrafo anterior lo escribi√≥ Github Copilot, es espectacular). Como es un modelo de clasificaci√≥n, este deber√≠a terminar con una sigmoide, pero se recomienda no hacerlo y utilizar una Loss Function de `BCEWithLogitsLoss`, que es una Binary CrossEntropy m√°s la sigmoide que provee mejor estabilidad num√©rica.

```python
class RecSys(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.criterion = nn.BCEWithLogitsLoss()
        
    def forward(self,users, movies):
        x = self.model(users, movies)
        return x
        
    def training_step(self, batch, batch_idx):
        users, movies, ratings = batch['users'], batch['movies'], batch['ratings']
        preds = self(users, movies)
        # print('preds:',  preds.shape)
        # print('ratings: ', ratings.shape)
        loss = self.criterion(preds, ratings.view(-1,1))
        self.log('train_loss', loss,  prog_bar = True, logger = True)
        return {'loss': loss}
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.model.parameters(), lr = 1e-3)
```

El `LightningModule` no tiene nada especial. S√≥lo mencionar que en este caso nuestra red neuronal recibe de cada batch los usuarios por un lado y las pel√≠culas por otro. Esto es importante porque usuarios y pel√≠culas tienen embeddings diferentes por lo que deben entrar al modelo por separado ya que los embeddings del modelo no van en serie sino en paralelo.

Finalmente para el entrenamiento instanciamos el modelo:

```python
model = NCF(dim_users, dim_movies)
dm = MovieDataModule(full_training_df, full_test_df, batch_size=512)
recommender = RecSys(model)
```
Definimos el Callback:

```python
mc = ModelCheckpoint(
    dirpath = 'checkpoints',
    #filename = 'best-checkpoint',
    save_last = True,
    save_top_k = 1,
    verbose = True,
    monitor = 'train_loss', 
    mode = 'min'
    )

mc.CHECKPOINT_NAME_LAST = 'best-checkpoint-latest'
```
Un detalle ac√° es que aprend√≠ que cambiando la variable `CHECKPOINT_NAME_LAST` se puede tener el mejor checkpoint con un nombre fijo. De esta manera podemos automatizar el rescate del mejor estado del modelo ya que siempre tendr√° el mismo nombre y no un `-v1`, `-v2`, `-v3`, etc.

Definimos el Trainer:

```python
trainer = pl.Trainer(max_epochs=5,
                    accelerator="gpu",
                    devices=1, 
                    callbacks=[mc], 
                    progress_bar_refresh_rate=30, 
                    # fast_dev_run=True,
                    #overfit_batches=1
                    )
trainer.fit(recommender, dm)
```
Ac√° es importante destacar que hay cambios, ahora para ejecutar en gpu no utilizamos `gpus=1` sino que definimos el `accelerator` que puede ser gpu, tpu, ipu, etc. Y definimos el `devices` que es el n√∫mero de GPUs que queremos utilizar.
Pueden ver comentado dos comandos que uso para debuggear que la red funcione correctamente antes de dejarla harto rato corriendo:

* `fast_dev_run=True` ejecuta una epoch de prueba para chequear por ejemplo que las dimensiones de los tensores funcionen bien.
* `overfit_batches=1` sobreajusta una batch por cada epoch. Si el overfit funciona es que el modelo efectivamente tiene la posibilidad de aprender. Con esto se puede chequear que el modelo no diverge.

Normalmente ejecuto esto antes de dejar la red corriendo y luego de una larga epoch de entrenamiento darme cuenta que fall√≥.

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    IPU available: False, using: 0 IPUs
    HPU available: False, using: 0 HPUs

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name      | Type              | Params
    ------------------------------------------------
    0 | model     | NCF               | 653 K 
    1 | criterion | BCEWithLogitsLoss | 0     
    ------------------------------------------------
    653 K     Trainable params
    0         Non-trainable params
    653 K     Total params
    2.612     Total estimated model params size (MB)



    Training: 0it [00:00, ?it/s]


    Epoch 0, global step 24996: 'train_loss' reached 0.08028 (best 0.08028), saving model to '/home/alfonso/Documents/kaggle/recom/checkpoints/epoch=0-step=24996.ckpt' as top 1
    Epoch 1, global step 49992: 'train_loss' was not in top 1
    Epoch 2, global step 74988: 'train_loss' reached 0.07823 (best 0.07823), saving model to '/home/alfonso/Documents/kaggle/recom/checkpoints/epoch=2-step=74988.ckpt' as top 1
    Epoch 3, global step 99984: 'train_loss' reached 0.06737 (best 0.06737), saving model to '/home/alfonso/Documents/kaggle/recom/checkpoints/epoch=3-step=99984.ckpt' as top 1
    Epoch 4, global step 124980: 'train_loss' reached 0.06487 (best 0.06487), saving model to '/home/alfonso/Documents/kaggle/recom/checkpoints/epoch=4-step=124980.ckpt' as top 1


### Evaluaci√≥n del Modelo

Otro cambio, esta vez en Pytorch 1.11, es la introducci√≥n de un decorador de inferencia. Hacer esto es equivalente al `with torch.no_grad()`.
B√°sicamente vaciaremos nuestro dataloader al modelo, aplicaremos la sigmoide que no colocamos en nuestro modelo e increiblemente descargando las predicciones a la CPU funcion√≥ m√°s r√°pido que la GPU (esto porque evita el overhead de subir los datos a la GPU para poder predecir).

```python
@torch.inference_mode()
def predict(model, dm):
    model.eval()
    preds = []
    for item in dm.test_dataloader():
        
        pred = torch.sigmoid(model(item['users'], item['movies']))
        preds.extend(pred.cpu().detach().numpy())
        
    return preds
```

Luego convertimos todo en un Numpy Array para poder incluirlo en nuestro `full_test_df`.

```python
predictions= np.array(predict(recommender, dm))
print(predictions.shape)
full_test_df['preds'] = predictions
full_test_df
```

    (6078000, 1)


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
      <th>preds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>734</th>
      <td>0</td>
      <td>230</td>
      <td>1.0</td>
      <td>0.987030</td>
    </tr>
    <tr>
      <th>1066</th>
      <td>1</td>
      <td>929</td>
      <td>1.0</td>
      <td>0.749257</td>
    </tr>
    <tr>
      <th>2855</th>
      <td>2</td>
      <td>465</td>
      <td>1.0</td>
      <td>0.911151</td>
    </tr>
    <tr>
      <th>2889</th>
      <td>3</td>
      <td>2505</td>
      <td>1.0</td>
      <td>0.973490</td>
    </tr>
    <tr>
      <th>3015</th>
      <td>4</td>
      <td>9907</td>
      <td>1.0</td>
      <td>0.959640</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6017215</th>
      <td>60779</td>
      <td>2442</td>
      <td>0.0</td>
      <td>0.006992</td>
    </tr>
    <tr>
      <th>6017216</th>
      <td>60779</td>
      <td>10800</td>
      <td>0.0</td>
      <td>0.003167</td>
    </tr>
    <tr>
      <th>6017217</th>
      <td>60779</td>
      <td>17767</td>
      <td>0.0</td>
      <td>0.000137</td>
    </tr>
    <tr>
      <th>6017218</th>
      <td>60779</td>
      <td>7073</td>
      <td>0.0</td>
      <td>0.000070</td>
    </tr>
    <tr>
      <th>6017219</th>
      <td>60779</td>
      <td>2124</td>
      <td>0.0</td>
      <td>0.000730</td>
    </tr>
  </tbody>
</table>
<p>6078000 rows √ó 4 columns</p>
</div>


Para evaluar el modelo utilizaremos la m√©trica HitRatio@10. Esto quiere decir que si dentro de las 10 mejores predicciones por usuario, el usuario tiene la pel√≠cula vista entonces eso es un √©xito.

```python
recomendations = full_test_df.sort_values(by = ['userId','preds'], ascending=[True, False]).groupby('userId').head(10)
```
{: title="C√°lculo 10 mejores predicciones por usuario"}

En uno de los art√≠culos que v√≠ como referencia encontr√© la siguiente implementaci√≥n:

```python
# User-item pairs for testing
test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))

# Dict of all items that are interacted with by each user
user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()

hits = []
for (u,i) in test_user_item_set:
    interacted_items = user_interacted_items[u]
    not_interacted_items = set(all_movieIds) - set(interacted_items)
    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))
    test_items = selected_not_interacted + [i]
    
    predicted_labels = np.squeeze(model(torch.tensor([u]*100), 
                                        torch.tensor(test_items)).detach().numpy())
    
    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]
    
    if i in top10_items:
        hits.append(1)
    else:
        hits.append(0)
        
print("The Hit Ratio @ 10 is {:.2f}".format(np.average(hits)))
```
S√≥lo al verla me dio dolor de estomago, porque no la entiendo. Una manera mucho m√°s sencilla es esta:

Sabemos que s√≥lo hay un 1 en cada usuario, por lo tanto `recomendations.rating.sum()` nos dir√° cuantas pel√≠culas efectivamente vistas por nuestro usuario est√°n en las 10 mejores recomendaciones. Si eso lo dividimos por el n√∫mero de usuarios entonces tenemos el HitRatio@10

```python
recomendations.rating.sum()/recomendations.userId.nunique()
```

    0.9457880881869036


## ¬øC√≥mo Utilizamos el Modelo?

Bueno para poder operacionalizar el modelo entonces tenemos que llevar a nuestros Ids originales:


```python
def back_to_normal(df, user_encoder, movie_encoder, movies_mapping):
    
    idx_movies = df.movieId.tolist()
    idx_users = df.userId.tolist()
    return pd.DataFrame(dict(userId = user_encoder.classes_[idx_users],
                    movieId = pd.Series(movie_encoder.classes_[idx_movies]).map(movies_mapping),
                    rating = df.rating.tolist()))
```


```python
vistos= back_to_normal(train_ratings, user_encoder, movie_encoder, movies_mapping)
visto.shape
```

    (2651157, 3)

```python
recomendar = back_to_normal(recomendations, user_encoder, movie_encoder, movies_mapping)
recomendar.shape
```
    (607800, 3)

`visto` corresponder√° a las pel√≠culas ya vistas por nuestro usuario, y `recomendar` a las 10 mejores recomendaciones. Ojo que agregamos el movies_mapping del principio para poder tener el nombre de la pel√≠cula y no s√≥lo el Id.

Revisemos entonces algunas recomendaciones:

{% include alert warning='Al revisar los resultados me d√≠ cuenta que s√© muy poco de pel√≠culas (a excepci√≥n de las pel√≠culas de Marvel) pido perd√≥n de antemano si mi an√°lisis es un poco pobre, pero no soy muy cin√©filo XD.'%}


Por ejemplo, el usuario 4 parece que le gustan las pel√≠culas de acci√≥n y ciencia ficci√≥n. Correctamente predijimos John Carter, que es la pel√≠cula que vio y adem√°s dado que ha visto varias pel√≠culas del MCU se le recomienda ver Thor 2 (que es muy mala pel√≠cula, pero bueno, nada que hacer).

```python
user = 4
print(visto.query('userId == @user')['movieId'])
recomendar.query('userId == @user')

```

    193                                Shutter Island (2010)
    194    Percy Jackson & the Olympians: The Lightning T...
    195                      How to Train Your Dragon (2010)
    196                           Clash of the Titans (2010)
    197                                    Iron Man 2 (2010)
                                 ...                        
    303             Spider-Man: Into the Spider-Verse (2018)
    304             John Wick: Chapter 3 ‚Äì Parabellum (2019)
    305                    Pok√©mon: Detective Pikachu (2019)
    306                               Ford v. Ferrari (2019)
    307         Fast & Furious Presents: Hobbs & Shaw (2019)
    Name: movieId, Length: 115, dtype: object


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10</th>
      <td>4</td>
      <td>Thor: The Dark World (2013)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>4</td>
      <td>Margin Call (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4</td>
      <td>Kubo and the Two Strings (2016)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>4</td>
      <td>John Carter (2012)</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>4</td>
      <td>Aut√≥mata (Automata) (2014)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>4</td>
      <td>You Were Never Really Here (2017)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>4</td>
      <td>Aloha (2015)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>4</td>
      <td>Thanks for Sharing (2012)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>4</td>
      <td>Eva (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>4</td>
      <td>Magic Mike XXL (2015)</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


El usuario 6225 parece que le gustan las pel√≠culas de Romance, Miedo y Suspenso, y se recomienda correctamente Midnight in Paris que no tengo idea de qu√© trata pero podemos ver otras recomendaciones como Saw (Miedo), Friends with Benefits o Aladdin que ser√°n medio Romance/Fantas√≠a supongo.

```python
user = 6265
print(visto.query('userId == @user')['movieId'])
recomendar.query('userId == @user')

```

    100326    Cabin in the Woods, The (2012)
    100327                Snowpiercer (2013)
    100328                  Gone Girl (2014)
    100329         The Imitation Game (2014)
    Name: movieId, dtype: object


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22630</th>
      <td>6265</td>
      <td>Midnight in Paris (2011)</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>22631</th>
      <td>6265</td>
      <td>Friends with Benefits (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22632</th>
      <td>6265</td>
      <td>Saw VII 3D - The Final Chapter (2010)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22633</th>
      <td>6265</td>
      <td>Searching (2018)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22634</th>
      <td>6265</td>
      <td>Aladdin (2019)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22635</th>
      <td>6265</td>
      <td>The Dark Tower (2017)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22636</th>
      <td>6265</td>
      <td>The BFG (2016)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22637</th>
      <td>6265</td>
      <td>ARQ (2016)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22638</th>
      <td>6265</td>
      <td>A Wrinkle in Time (2018)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22639</th>
      <td>6265</td>
      <td>Magic of Belle Isle, The (2012)</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


El usuario 63 es como mi esposa, le gustan las pel√≠culas livianitas, de monitos o para reirse, Tangled, Inside Out o Pitch Perfect s√≥lo pueden recomendar algo como The Twilight Saga: Eclipse.

```python
user = 63
print(visto.query('userId == @user')['movieId'])
recomendar.query('userId == @user')

```

    623                              Easy A (2010)
    624                             Tangled (2010)
    625                         Bridesmaids (2011)
    626                     Horrible Bosses (2011)
    627                Crazy, Stupid, Love. (2011)
    628                      21 Jump Street (2012)
    629                       Pitch Perfect (2012)
    630    Perks of Being a Wallflower, The (2012)
    631                   Great Gatsby, The (2013)
    632                      Now You See Me (2013)
    633                   We're the Millers (2013)
    634                          About Time (2013)
    635            Wolf of Wall Street, The (2013)
    636                           Gone Girl (2014)
    637                          Inside Out (2015)
    638                                Room (2015)
    639                               Moana (2016)
    640                                Coco (2017)
    Name: movieId, dtype: object





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>200</th>
      <td>63</td>
      <td>Spotlight (2015)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>201</th>
      <td>63</td>
      <td>Twilight Saga: Eclipse, The (2010)</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>202</th>
      <td>63</td>
      <td>Sorcerer's Apprentice, The (2010)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>203</th>
      <td>63</td>
      <td>Melancholia (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>204</th>
      <td>63</td>
      <td>Oz the Great and Powerful (2013)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>205</th>
      <td>63</td>
      <td>Venom (2018)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>206</th>
      <td>63</td>
      <td>Selma (2014)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>207</th>
      <td>63</td>
      <td>Burlesque (2010)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>208</th>
      <td>63</td>
      <td>Silent Hill: Revelation 3D (2012)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>209</th>
      <td>63</td>
      <td>Double, The (2011)</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


El usuario 162532 es de los m√≠os, harta pel√≠cula de Acci√≥n, del MCU y como de Ni√±os (Despicable Me) y se recomienda Guardians of the Galaxy (muy buena pel√≠cula) y Spy Kids que es de Acci√≥n e Infantil que tambi√©n es rebuena.
```python
user = 162532
print(visto.query('userId == @user')['movieId'])
recomendar.query('userId == @user')

```

    2650878                      How to Train Your Dragon (2010)
    2650879                                      Kick-Ass (2010)
    2650880                    Exit Through the Gift Shop (2010)
    2650881                                    Iron Man 2 (2010)
    2650882                                 Despicable Me (2010)
    2650883                                     Inception (2010)
    2650884                   Scott Pilgrim vs. the World (2010)
    2650885                           Social Network, The (2010)
    2650886                                        Easy A (2010)
    2650887    Harry Potter and the Deathly Hallows: Part 1 (...
    2650888                            King's Speech, The (2010)
    2650889                                   Source Code (2011)
    2650890                                          Thor (2011)
    2650891                            X-Men: First Class (2011)
    2650892    Harry Potter and the Deathly Hallows: Part 2 (...
    2650893            Captain America: The First Avenger (2011)
    2650894                                 Avengers, The (2012)
    2650895                                          Hugo (2011)
    2650896                              The Hunger Games (2012)
    2650897                        Dark Knight Rises, The (2012)
    2650898            Sherlock Holmes: A Game of Shadows (2011)
    2650899                                  Intouchables (2011)
    2650900                                        Looper (2012)
    2650901                                          Argo (2012)
    2650902                       Silver Linings Playbook (2012)
    2650903            Hobbit: An Unexpected Journey, The (2012)
    2650904                                    Iron Man 3 (2013)
    Name: movieId, dtype: object





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>607750</th>
      <td>162532</td>
      <td>Guardians of the Galaxy (2014)</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>607751</th>
      <td>162532</td>
      <td>Only the Brave (2017)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607752</th>
      <td>162532</td>
      <td>Immigrant, The (2013)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607753</th>
      <td>162532</td>
      <td>Diary of a Wimpy Kid: Rodrick Rules (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607754</th>
      <td>162532</td>
      <td>Spy Kids: All the Time in the World in 4D (2011)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607755</th>
      <td>162532</td>
      <td>The Belko Experiment (2017)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607756</th>
      <td>162532</td>
      <td>All the Way (2016)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607757</th>
      <td>162532</td>
      <td>Come Together (2016)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607758</th>
      <td>162532</td>
      <td>Batman: Gotham by Gaslight (2018)</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>607759</th>
      <td>162532</td>
      <td>Kizumonogatari Part 1: Tekketsu (2016)</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


## Conclusiones

Creo que el Modelo funciona sumamente bien. Cabe destacar que estamos haciendo el trabajo bien dificil porque en estricto rigor nosotros deber√≠amos predecir el Rating de todas las pel√≠culas del cat√°logo y entregar las 10 mejores, y estamos haciendo un random de 99 pel√≠culas que puede que no tengan nada que ver con el usuario y a√∫n as√≠ el modelo es capaz de ordenar las predicciones de buena manera.

Es interesante que gran parte del modelo de Recomendaci√≥n tiene que ver con el manejo de la data y c√≥mo vamos a operacionalizarlo. No es llegar y hacer un predict sino que es necesario pensar en una estrategia para poder mostrar esto. 

Una de las ventajas de este tipo de modelo es que podemos tener todas las predicciones hechas por ejemplo en la noche y luego operacionalizarlas en nuestro front-end. Esto es beneficioso tambi√©n en el sentido que una inferencia en tiempo real para semejante cantidad de datos es dif√≠cil.

Una desventaja de este tipo de modelos es que s√≥lo pueden entregar recomendaciones a los usuarios que ya han visto pel√≠culas y que ya se encuentran en la user-item matrix. Esto es lo que se conoce como el cold-start problem. No s√© muy bien como se soluciona pero en la forma en la que se plante√≥ esta soluci√≥n no es posible decir me gusta A, B, C, entonces, ¬øqu√© me recomiendas? Deben existir otro tipo de modelos que s√≠ pueden lidiar con esto, pero que no manejo.

Eso es todo por esta semana, espero les haya gustado y gracias Gustavo, aprend√≠ harto de Sistemas Recomendadores (aunque me sac√≥ canas verdes) durante este fin de semana.

Hasta la otra, 

[**Alfonso**]({{ site.baseurl }}/contact/)
