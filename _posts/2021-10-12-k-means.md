---
title: "Compresi√≥n de Im√°genes usando Machine Learning"
subheadline: "Aprendizaje No Supervisado m√°s all√° de Segmentaciones."
teaser: "K-Means en Scikit-Learn"
# layout: page-fullwidth
usemathjax: true
category: quick
header: no
image:
    thumb: k-means/compression690.png
tags:
- ML
- sklearn
- tutorial
published: true
---


![picture of me]({{ site.urlimg }}k-means/compression690.png){: .left .show-for-large-up .hide-for-print width="500"}
![picture of me]({{ site.urlimg }}k-means/compression690.png){: .center .hide-for-large-up width="250"}

En un pa√≠s donde las ofertas de puestos de Data Scientist aparecen por todas partes es muy importante estar preparado. Me llama especialmente la atenci√≥n que hay muy pocas entrevistas t√©cnicas que eval√∫an el conocimiento de los Data Scientist a contratar. Esto puede ser √∫til si eres nuevo, porque puede ser m√°s sencillo entrar el mundillo este. Pero eventualmente, debemos llegar a est√°ndares m√°s altos donde se cuestionar√°n las habilidades que uno tiene. Siempre se cuentan los casos de √©xitos, pero yo quiero contar mi m√°ximo **FAIL**.<!--more-->

Estaba postulando a una empresa s√∫per prestigiosa en t√©rminos de Machine Learning, y la verdad es que iba bien preparado a la entrevista. Hab√≠a pasado una prueba bien dura de programaci√≥n, y me tocaba una entrevista de teor√≠a. Siendo honesto, no estudi√© tanto, porque ya estaba full haciendo clases y consideraba que ten√≠a buena preparaci√≥n te√≥rica. Y bueno, pens√© que lo m√°s complejo que me pod√≠an preguntar era qu√© es el <q>p-value</q> (que creo que no s√© como explicar). 

Llegu√© a la entrevista, eran 10 preguntas, yo sent√≠ que estaba contestando bien, hasta que en la pregunta 9 me dice vamos a hablar de algoritmos no supervisados. Y me entr√≥ el escalofr√≠o. Se usa poco en la pr√°ctica (al menos bastante menos que los algoritmos supervisados). Con suerte se usa para hacer segmentaciones. Y la verdad no lo tengo para nada en memoria. Me hace la primera pregunta: ¬øQu√© precauciones debemos tener antes de usar K-Means? Siento que la respond√≠ bastante bien, me hab√≠a tocado hacer segmentaciones por lo que ten√≠a super claro como preprocesar la data y en qu√© fijarme al momento de utilizarlo. **Pregunta 10:** Expl√≠came en detalle, cada paso del algoritmo de K-Means. üò± En ese momento me d√≠ cuenta de que me falt√≥ estudiar. Ten√≠a una idea en la cabeza porque hab√≠a implementado un K-means desde cero en Python puro cuando d√≠ el XCS229 del Programa de Inteligencia Articial que tom√© en Stanford (el cual recomiendo totalmente si es que les interesa la parte te√≥rica del ML), pero no me pod√≠a acordar c√≥mo se actualizaba el centro de cada cluster. Expliqu√© todo lo que sab√≠a, evitando (m√°s bien salt√°ndome) lo que no recordaba. Entonces, el entrevistador me dijo: Oye, est√° super bien, pero no me queda claro c√≥mo se actualizan los centros de cada cluster: üò® Me pill√≥. En las reglas de la entrevista dec√≠a que si uno no sab√≠a algo pod√≠a decir que no sab√≠a, as√≠ que le dije: <q>Pucha, no recuerdo esa parte.</q>. No me llamaron m√°s (as√≠ que asum√≠ que no qued√© üòá).

Si bien este tipo de entrevistas no son tan comunes, cuando quieres postular a empresas que se dedican a hacer Machine Learning en serio (y no s√≥lo a hacer los t√≠picos modelos de Propensi√≥n, Churn, Segmentaciones, etc.) hay que entender el proceso a fondo. Y en este caso el no recordar que los centroides se actualizan promediando los puntos del cl√∫ster (por eso K-Means) me cost√≥ no quedar en el puesto (lo cual todav√≠a lamento).

## El algoritmo de K-Means

La verdad es que este algoritmo es bien sencillito. B√°sicamente tienes puntos en un espacio y quieres armar comunidades (cl√∫sters) de puntos que se parezcan, es decir que sean cercanos. Para ello se siguen los siguientes pasos:

* Se decide el n√∫mero de Clusters. K-Means no es capaz de decidir a priori cu√°ntos clusters utilizar. Por lo tanto , √©stos se tienen que dar de antemano (es decir, es un hiperpar√°metro). Las coordenadas para cada centroide del cluster se inicializar√°n de manera aleatoria.
* Cada punto es asignado a un Cluster. Para esto se define cu√°l es la distancia de cada punto a cada centroide y se asignar√° al cluster que tenga la menor distancia a su centroide.
* Se actualizan los centroides (mi fail). Se toman todos los puntos pertenecientes a un cluster y se promedian, este resultado corresponder√° a las coordenadas del nuevo centroide.
* Se repiten los pasos 2 y 3 hasta lograr la convergencia. La convergencia se logra cuando ya no hay variaci√≥n de los centroides entre cada iteraci√≥n.

![picture of me]({{ site.urlimg }}k-means/k-means-algo.png){: .center }

En fin, no es necesario aplicar este algoritmo **"from scratch"**, ya que de hecho este es un problema de complejidad NP-Hard (ba≈õicamente, d√≠ficil de optimizar) y `Scikit-Learn` tiene una implementaci√≥n muy √≥ptimizada y f√°cil de usar.

Ahora la aplicaci√≥n que quiero mostrar es una bien interesante de "Image Compression". B√°sicamente bajar la calidad de la im√°gen. Para ello haremos lo siguiente:

```python
from sklearn.cluster import KMeans
from matplotlib.image import imread
import pandas as pd
import matplotlib.pyplot as pltProbando nuevos t√≠tulos
```
{: title="Importar Librer√≠as"}


```python
full_resolution_img = imread('peppers-large.tiff')
plt.figure(figsize = (10,10))
plt.imshow(full_resolution_img)
plt.axis('off');
```
{: title="Importar imagen"}

![png]({{ site.urlimg }}k-means/output_0_0.png){: .center}
    

Si les interesa replicar el procedimiento pueden descargar la im√°gen [ac√°](https://drive.google.com/file/d/1O-1MpfdX1c1mzDfMEiob4Vyr9A1qwOFD/view?usp=sharing).

```python
full_resolution_img.shape
```
    (512, 512, 3)

Como se puede ver, se trata de una imagen de 3 canales (RGB) de resoluci√≥n 512x512 pixeles. Qu√© pasar√≠a, si en vez de tratar la imagen en este formato la cambiamos de la siguiente manera:


```python
pixels = full_resolution_img.reshape(-1,3)/255
pixels.shape
```

    (262144, 3)

En este caso obtenemos una lista de cada uno de los pixeles. Podr√≠amos decir entonces que luego del `reshape` tenemos un dataset con todos los colores (combinaciones de Rojo, Verde y Azul) utilizados en la imagen. 

{% include alert info='La divisi√≥n por 255 es para poder tener todos los valores en el rango 0 a 1 para facilitar la convergencia al momento de hacer el K-Means.'%}

Ac√° es donde entra el K-Means. Podemos de cierta manera generar clusters, que pueden ser interpretados como agrupaciones de colores. Esto resultar√° en modificar nuestra imagen original de 24 bits a una versi√≥n m√°s comprimida.

{% include alert tip='24 bits es la cantidad de colores distintos que se pueden generar en un formato RGB. Esto nace debido que cada pixel puede tomar 256 valores de 0 a 255. Esto es 2‚Å∏, lo cual se define como 8 bits. Dado que tenemos 3 canales de 8 bits, se dice que la imagen es de 24 bits.'%}

Para hacer la compresi√≥n haremos entonces 16 clusters. Esto nos dar√° 4 bits por canal, por lo tanto tendr√≠amos im√°genes de 12 bits.

```python
kmeans = KMeans(n_clusters=16, random_state=123)
cluster = kmeans.fit_predict(pixels)
cluster
```
{: title="K-Means para comprimir la imagen."}

    array([ 6, 14, 14, ...,  5,  2,  2], dtype=int32)


```python
pd.Series(cluster).value_counts()
```
{: title="Resultado del Clustering"}


    13    30027
    7     30020
    1     25541
    10    24267
    4     22099
    11    19800
    15    16150
    3     15094
    0     14131
    12    13174
    6     12862
    5     11570
    2     10958
    8     10471
    9      5475
    14      505
    dtype: int64

Como se puede apreciar, ahora s√≥lo tenemos 16 clusters. Estos 16 clusters podr√≠an representar 16 colores principales, que por la definici√≥n del K-Means ser√°n el promedio de los colores del cluster. Como s√≥lo pueden haber 16 valores distintos por cada canal, eso es 2‚Å¥, luego, tenemos 4 bits. Ahora, no nos interesa el identificador del cluster que es el resultado del `.fit_predict()`, m√°s bien, nos interesan los centroides de cada cluster, ya que como bien dijimos, estos representar√°n un color promedio. Para obtener esto en `Scikit-Learn` hacemos lo siguiente:

```python
centers = kmeans.cluster_centers_
centers
```
{: title="Obtenci√≥n de Centroides."}


    array([[4.59789396e-01, 4.15149528e-01, 2.04456932e-01],
           [7.22265979e-01, 1.58838576e-01, 1.48959576e-01],
           [7.58603080e-01, 8.57161927e-01, 7.55448074e-01],
           [1.82948346e-01, 6.77831811e-03, 4.25771793e-03],
           [6.08454512e-01, 7.00841351e-01, 3.05950520e-01],
           [7.19107297e-01, 7.99673203e-01, 5.67397973e-01],
           [3.45444096e-01, 4.45862526e-02, 4.21471318e-02],
           [7.25751102e-01, 7.89391203e-01, 3.47212740e-01],
           [4.43207003e-01, 2.40346951e-01, 1.42962325e-01],
           [7.92388091e-01, 4.49669623e-01, 2.95399065e-01],
           [4.73568103e-01, 6.75029949e-01, 3.31192368e-01],
           [4.47121988e-01, 5.61503086e-01, 2.56008633e-01],
           [5.50247767e-01, 7.49612934e-01, 4.39358419e-01],
           [7.97094883e-01, 2.23587943e-01, 1.71642967e-01],
           [5.96233741e-01, 1.49880108e-15, 7.21871481e-01],
           [5.70648315e-01, 8.48032237e-02, 9.75248360e-02]])

Finalmente, lo que nosotros queremos es que cada color original sea reemplazado con las coordenadas del centroide de su cluster. Esto se puede lograr de manera muy sencilla tan solo incluyendo los clusters dentro del dataset de centros. Esto puede ser raro al inicio, y es m√°s bien un truco matricial. La mejor manera de pensarlo es como que el id del cluster es el √≠ndice de cada centroide, y voy a llamar a dichas coordenadas en el orden en el que result√≥ mi clustering (output del `fit_predict()`).

Finalmente para reconstruir la imagen tengo que volverlo a la forma original de 512x512 y listo!!

```python
new_image = centers[cluster].reshape(512,512,3)
plt.figure(figsize = (10,10))
plt.imshow(new_image)
plt.axis('off');
```
    
![png]({{ site.urlimg }}k-means/output_7_0.png){: .center}

```python
fig, ax = plt.subplots(1,2, figsize = (20,20))
ax[0].imshow(full_resolution_img)
ax[0].axis('off')
ax[1].imshow(new_image)
ax[1].axis('off')
```

![png]({{ site.urlimg }}k-means/output_8_1.png){: .center}

{% include alert success='Como se puede apreciar en la imagen, la foto de la derecha tiene un cambio de colores mucho m√°s tosco, no tan "smooth" como el de la izquierda. Esto debido a que posee una cantidad menor de colores. La nitidez y la calidad del color se ve afectado pero, la imagen sigue siendo la misma. Por lo tanto, este podr√≠a ser un buen preprocesamiento al momento de tratar im√°genes para poder rescatar la informaci√≥n importante, pero a un costo computacional menor.'%}

Espero, que con esto no se me olvide nunca m√°s el funcionamiento del algoritmo de K-means. Y tambien espero que les haya parecido interesante.

Nos vemos a la otra,

[**Alfonso**]({{ site.baseurl }}/contact/)

